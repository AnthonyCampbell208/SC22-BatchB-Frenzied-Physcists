{"backend_state":"running","connection_file":"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/share/jupyter/runtime/kernel-4415a3be-5eb6-4f84-8c4d-c4b685a377ae.json","kernel":"ds_env","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1657722975075,"exec_count":1,"id":"98ba1f","input":"import pandas as pd\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport sklearn\nfrom sklearn import tree\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier as ADC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.linear_model import LogisticRegression as LR\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import BallTree\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import GridSearchCV as GSearch\n\nfrom imblearn.under_sampling import RandomUnderSampler ","kernel":"ds_env","pos":0,"start":1657722973390,"state":"done","type":"cell"}
{"cell_type":"code","end":1657722975532,"exec_count":2,"id":"6a663d","input":"airline_df = pd.read_csv('./data/airline_data.csv')\nairline_df.dropna(inplace=True)\nairline_df.reset_index(drop=True, inplace=True)\nairline_df = pd.get_dummies(airline_df, prefix = None, prefix_sep = '_', dummy_na = False, columns = ['satisfaction','Gender', 'Customer Type', 'Type of Travel', 'Class'], sparse = False, drop_first = False, dtype = None)\ncolumns_drop = ['id', 'Unnamed: 0.1', 'Unnamed: 0', 'satisfaction_neutral or dissatisfied']\nairline_df.drop(columns_drop, axis=1, inplace = True)\n","kernel":"ds_env","pos":1,"start":1657722975098,"state":"done","type":"cell"}
{"cell_type":"code","end":1657722975560,"exec_count":3,"id":"0fe4d1","input":"airline_df.isnull().sum()","kernel":"ds_env","output":{"0":{"data":{"text/plain":"Age                                  0\nFlight Distance                      0\nInflight wifi service                0\nDeparture/Arrival time convenient    0\nEase of Online booking               0\nGate location                        0\nFood and drink                       0\nOnline boarding                      0\nSeat comfort                         0\nInflight entertainment               0\nOn-board service                     0\nLeg room service                     0\nBaggage handling                     0\nCheckin service                      0\nInflight service                     0\nCleanliness                          0\nDeparture Delay in Minutes           0\nArrival Delay in Minutes             0\nsatisfaction_satisfied               0\nGender_Female                        0\nGender_Male                          0\nCustomer Type_Loyal Customer         0\nCustomer Type_disloyal Customer      0\nType of Travel_Business travel       0\nType of Travel_Personal Travel       0\nClass_Business                       0\nClass_Eco                            0\nClass_Eco Plus                       0\ndtype: int64"},"exec_count":3}},"pos":2,"start":1657722975538,"state":"done","type":"cell"}
{"cell_type":"code","end":1657722975569,"exec_count":4,"id":"6c6158","input":"airline_df.columns","kernel":"ds_env","output":{"0":{"data":{"text/plain":"Index(['Age', 'Flight Distance', 'Inflight wifi service',\n       'Departure/Arrival time convenient', 'Ease of Online booking',\n       'Gate location', 'Food and drink', 'Online boarding', 'Seat comfort',\n       'Inflight entertainment', 'On-board service', 'Leg room service',\n       'Baggage handling', 'Checkin service', 'Inflight service',\n       'Cleanliness', 'Departure Delay in Minutes', 'Arrival Delay in Minutes',\n       'satisfaction_satisfied', 'Gender_Female', 'Gender_Male',\n       'Customer Type_Loyal Customer', 'Customer Type_disloyal Customer',\n       'Type of Travel_Business travel', 'Type of Travel_Personal Travel',\n       'Class_Business', 'Class_Eco', 'Class_Eco Plus'],\n      dtype='object')"},"exec_count":4}},"pos":3,"start":1657722975566,"state":"done","type":"cell"}
{"cell_type":"code","end":1657722975622,"exec_count":5,"id":"2616e8","input":"airline_df.head()","kernel":"ds_env","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Flight Distance</th>\n      <th>Inflight wifi service</th>\n      <th>Departure/Arrival time convenient</th>\n      <th>Ease of Online booking</th>\n      <th>Gate location</th>\n      <th>Food and drink</th>\n      <th>Online boarding</th>\n      <th>Seat comfort</th>\n      <th>Inflight entertainment</th>\n      <th>...</th>\n      <th>satisfaction_satisfied</th>\n      <th>Gender_Female</th>\n      <th>Gender_Male</th>\n      <th>Customer Type_Loyal Customer</th>\n      <th>Customer Type_disloyal Customer</th>\n      <th>Type of Travel_Business travel</th>\n      <th>Type of Travel_Personal Travel</th>\n      <th>Class_Business</th>\n      <th>Class_Eco</th>\n      <th>Class_Eco Plus</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>13</td>\n      <td>460</td>\n      <td>3</td>\n      <td>4</td>\n      <td>3</td>\n      <td>1</td>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>25</td>\n      <td>235</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26</td>\n      <td>1142</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>25</td>\n      <td>562</td>\n      <td>2</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>61</td>\n      <td>214</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>4</td>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 28 columns</p>\n</div>","text/plain":"   Age  Flight Distance  Inflight wifi service  \\\n0   13              460                      3   \n1   25              235                      3   \n2   26             1142                      2   \n3   25              562                      2   \n4   61              214                      3   \n\n   Departure/Arrival time convenient  Ease of Online booking  Gate location  \\\n0                                  4                       3              1   \n1                                  2                       3              3   \n2                                  2                       2              2   \n3                                  5                       5              5   \n4                                  3                       3              3   \n\n   Food and drink  Online boarding  Seat comfort  Inflight entertainment  ...  \\\n0               5                3             5                       5  ...   \n1               1                3             1                       1  ...   \n2               5                5             5                       5  ...   \n3               2                2             2                       2  ...   \n4               4                5             5                       3  ...   \n\n   satisfaction_satisfied  Gender_Female  Gender_Male  \\\n0                       0              0            1   \n1                       0              0            1   \n2                       1              1            0   \n3                       0              1            0   \n4                       1              0            1   \n\n   Customer Type_Loyal Customer  Customer Type_disloyal Customer  \\\n0                             1                                0   \n1                             0                                1   \n2                             1                                0   \n3                             1                                0   \n4                             1                                0   \n\n   Type of Travel_Business travel  Type of Travel_Personal Travel  \\\n0                               0                               1   \n1                               1                               0   \n2                               1                               0   \n3                               1                               0   \n4                               1                               0   \n\n   Class_Business  Class_Eco  Class_Eco Plus  \n0               0          0               1  \n1               1          0               0  \n2               1          0               0  \n3               1          0               0  \n4               1          0               0  \n\n[5 rows x 28 columns]"},"exec_count":5}},"pos":4,"start":1657722975575,"state":"done","type":"cell"}
{"cell_type":"code","end":1657722975718,"exec_count":6,"id":"96f91b","input":"\nairline_df.dropna(inplace=True)\nairline_df.reset_index(drop=True, inplace=True)","kernel":"ds_env","pos":5,"start":1657722975668,"state":"done","type":"cell"}
{"cell_type":"code","end":1657722975747,"exec_count":7,"id":"4272fa","input":"reduced_df = airline_df.sample(frac=0.10, random_state=42)\nreduced_df.shape","kernel":"ds_env","output":{"0":{"data":{"text/plain":"(12949, 28)"},"exec_count":7}},"pos":6,"start":1657722975735,"state":"done","type":"cell"}
{"cell_type":"code","end":1657722975759,"exec_count":8,"id":"802ae3","input":"reduced_df.satisfaction_satisfied.value_counts()","kernel":"ds_env","output":{"0":{"data":{"text/plain":"0    7323\n1    5626\nName: satisfaction_satisfied, dtype: int64"},"exec_count":8}},"pos":7,"start":1657722975751,"state":"done","type":"cell"}
{"cell_type":"code","end":1657722975768,"exec_count":9,"id":"469a0a","input":"target = reduced_df['satisfaction_satisfied']","kernel":"ds_env","pos":8,"start":1657722975766,"state":"done","type":"cell"}
{"cell_type":"code","end":1657722975778,"exec_count":10,"id":"752ae3","input":"input_columns = reduced_df.loc[:, airline_df.columns != \"satisfaction_satisfied\"]","kernel":"ds_env","pos":9,"start":1657722975771,"state":"done","type":"cell"}
{"cell_type":"code","end":1657722975786,"exec_count":11,"id":"339596","input":"input_columns.shape","kernel":"ds_env","output":{"0":{"data":{"text/plain":"(12949, 27)"},"exec_count":11}},"pos":10,"start":1657722975780,"state":"done","type":"cell"}
{"cell_type":"code","end":1657722975807,"exec_count":12,"id":"da00d9","input":"x_train, x_test, y_train, y_test = train_test_split(input_columns, target, stratify=target, train_size=0.8)","kernel":"ds_env","pos":11,"start":1657722975794,"state":"done","type":"cell"}
{"cell_type":"code","end":1657723324631,"exec_count":15,"id":"dedda0","input":"sgd_model = SGDClassifier()\nsgd_model.fit(x_train, y_train)\ny_pred_sgd = sgd_model.predict(x_test)\nacc = accuracy_score(y_test, y_pred_sgd)\nprec = precision_score(y_test, y_pred_sgd)\nrecall = recall_score(y_test, y_pred_sgd)\nf1 = f1_score(y_test, y_pred_sgd)\nscores['sgd'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\n\nparam_grid = {'loss': [\"hinge\", \"log_loss\", \"modified_huber\", \"squared_hinge\", \"perceptron\"],  \n              'penalty': [\"l2\", \"l1\", \"elasticnet\"], \n              'max_iter':[1000, 10000, 30000],\n              'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0]}  \n   \ngrid = GridSearchCV(SGDClassifier(), param_grid, refit = True, verbose = 3,n_jobs=-1) \n   \ngrid.fit(x_train, y_train) \n \nprint(grid.best_params_) \n\nbest_estimator = grid.best_estimator_\ny_pred_sgd = best_estimator.predict(x_test)\nacc = accuracy_score(y_test, y_pred_sgd)\nprec = precision_score(y_test, y_pred_sgd)\nrecall = recall_score(y_test, y_pred_sgd)\nf1 = f1_score(y_test, y_pred_sgd)\nscores['sgd grid'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}","kernel":"ds_env","output":{"0":{"name":"stdout","text":"Fitting 5 folds for each of 225 candidates, totalling 1125 fits\n"},"1":{"name":"stdout","text":"[CV 2/5] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=l2;, score=0.571 total time=   0.2s\n[CV 4/5] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=l2;, score=0.602 total time=   0.2s\n[CV 5/5] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=l2;, score=0.592 total time=   0.2s\n[CV 2/5] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=l1;, score=0.803 total time=   0.3s\n[CV 4/5] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=l1;, score=0.766 total time=   0.5s\n[CV 5/5] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=l1;, score=0.798 total time=   0.3s\n[CV 2/5] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.757 total time=   0.4s\n[CV 4/5] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.810 total time=   0.5s\n[CV 1/5] END alpha=0.0001, loss=hinge, max_iter=10000, penalty=l2;, score=0.625 total time=   0.2s\n[CV 3/5] END alpha=0.0001, loss=hinge, max_iter=10000, penalty=l2;, score=0.573 total time=   0.2s\n[CV 1/5] END alpha=0.0001, loss=hinge, max_iter=10000, penalty=l1;, score=0.775 total time=   0.3s\n[CV 3/5] END alpha=0.0001, loss=hinge, max_iter=10000, penalty=l1;, score=0.758 total time=   0.3s\n[CV 1/5] END alpha=0.0001, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.556 total time=   0.4s\n[CV 3/5] END alpha=0.0001, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.511 total time=   0.4s\n[CV 4/5] END alpha=0.0001, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.824 total time=   0.4s\n[CV 3/5] END alpha=0.0001, loss=hinge, max_iter=30000, penalty=l2;, score=0.568 total time=   0.3s\n[CV 4/5] END alpha=0.0001, loss=hinge, max_iter=30000, penalty=l2;, score=0.488 total time=   0.2s\n[CV 1/5] END alpha=0.0001, loss=hinge, max_iter=30000, penalty=l1;, score=0.728 total time=   0.3s\n[CV 3/5] END alpha=0.0001, loss=hinge, max_iter=30000, penalty=l1;, score=0.788 total time=   0.2s\n[CV 5/5] END alpha=0.0001, loss=hinge, max_iter=30000, penalty=l1;, score=0.782 total time=   0.4s\n[CV 3/5] END alpha=0.0001, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.826 total time=   0.5s\n[CV 5/5] END alpha=0.0001, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.573 total time=   0.5s\n[CV 2/5] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2;, score=0.580 total time=   0.2s\n[CV 4/5] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2;, score=0.750 total time=   0.2s\n[CV 5/5] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2;, score=0.467 total time=   0.2s\n[CV 2/5] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1;, score=0.804 total time=   0.3s\n[CV 4/5] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1;, score=0.782 total time=   0.4s\n[CV 1/5] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet;, score=0.557 total time=   0.6s\n[CV 3/5] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet;, score=0.825 total time=   0.5s\n[CV 5/5] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet;, score=0.656 total time=   0.4s\n[CV 3/5] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2;, score=0.542 total time=   0.2s\n[CV 5/5] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2;, score=0.521 total time=   0.2s\n[CV 2/5] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1;, score=0.809 total time=   0.4s\n[CV 4/5] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1;, score=0.710 total time=   0.5s\n[CV 1/5] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet;, score=0.786 total time=   0.4s\n[CV 3/5] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet;, score=0.518 total time=   0.5s\n[CV 5/5] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet;, score=0.786 total time=   0.4s\n[CV 2/5] END alpha=0.0001, loss=log_loss, max_iter=30000, penalty=l2;, score=0.575 total time=   0.2s\n[CV 5/5] END alpha=0.0001, loss=log_loss, max_iter=30000, penalty=l2;, score=0.573 total time=   0.2s\n[CV 2/5] END alpha=0.0001, loss=log_loss, max_iter=30000, penalty=l1;, score=0.675 total time=   0.3s\n[CV 3/5] END alpha=0.0001, loss=log_loss, max_iter=30000, penalty=l1;, score=0.794 total time=   0.6s\n[CV 1/5] END alpha=0.0001, loss=log_loss, max_iter=30000, penalty=elasticnet;, score=0.508 total time=   0.5s\n[CV 3/5] END alpha=0.0001, loss=log_loss, max_iter=30000, penalty=elasticnet;, score=0.801 total time=   0.4s\n[CV 4/5] END alpha=0.0001, loss=log_loss, max_iter=30000, penalty=elasticnet;, score=0.491 total time=   0.4s\n[CV 1/5] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=l2;, score=0.573 total time=   0.2s\n[CV 2/5] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=l2;, score=0.604 total time=   0.2s\n[CV 3/5] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=l2;, score=0.568 total time=   0.2s\n[CV 5/5] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=l2;, score=0.484 total time=   0.2s\n[CV 2/5] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=l1;, score=0.760 total time=   0.3s\n[CV 4/5] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=l1;, score=0.778 total time=   0.4s\n[CV 1/5] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=elasticnet;, score=0.474 total time=   0.4s\n[CV 3/5] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=elasticnet;, score=0.573 total time=   0.4s\n[CV 5/5] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=elasticnet;, score=0.714 total time=   0.3s\n[CV 2/5] END alpha=0.0001, loss=modified_huber, max_iter=10000, penalty=l2;, score=0.683 total time=   0.1s\n[CV 4/5] END alpha=0.0001, loss=modified_huber, max_iter=10000, penalty=l2;, score=0.495 total time=   0.1s\n[CV 1/5] END alpha=0.0001, loss=modified_huber, max_iter=10000, penalty=l1;, score=0.718 total time=   0.3s\n[CV 3/5] END alpha=0.0001, loss=modified_huber, max_iter=10000, penalty=l1;, score=0.792 total time=   0.3s\n[CV 4/5] END alpha=0.0001, loss=modified_huber, max_iter=10000, penalty=l1;, score=0.773 total time=   0.3s\n[CV 1/5] END alpha=0.0001, loss=modified_huber, max_iter=10000, penalty=elasticnet;, score=0.568 total time=   0.3s\n[CV 3/5] END alpha=0.0001, loss=modified_huber, max_iter=10000, penalty=elasticnet;, score=0.696 total time=   0.4s\n[CV 5/5] END alpha=0.0001, loss=modified_huber, max_iter=10000, penalty=elasticnet;, score=0.484 total time=   0.2s\n[CV 3/5] END alpha=0.0001, loss=modified_huber, max_iter=30000, penalty=l2;, score=0.803 total time=   0.2s\n[CV 5/5] END alpha=0.0001, loss=modified_huber, max_iter=30000, penalty=l2;, score=0.552 total time=   0.2s\n[CV 2/5] END alpha=0.0001, loss=modified_huber, max_iter=30000, penalty=l1;, score=0.795 total time=   0.4s\n[CV 4/5] END alpha=0.0001, loss=modified_huber, max_iter=30000, penalty=l1;, score=0.773 total time=   0.2s\n[CV 5/5] END alpha=0.0001, loss=modified_huber, max_iter=30000, penalty=l1;, score=0.773 total time=   0.5s\n[CV 2/5] END alpha=0.0001, loss=modified_huber, max_iter=30000, penalty=elasticnet;, score=0.727 total time=   0.4s\n[CV 5/5] END alpha=0.0001, loss=modified_huber, max_iter=30000, penalty=elasticnet;, score=0.569 total time=   0.3s\n[CV 1/5] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=l2;, score=0.769 total time=   0.1s\n[CV 3/5] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=l2;, score=0.575 total time=   0.2s\n[CV 5/5] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=l2;, score=0.730 total time=   0.3s\n[CV 2/5] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=l1;, score=0.754 total time=   0.3s\n[CV 4/5] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=l1;, score=0.780 total time=   0.3s\n[CV 1/5] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=elasticnet;, score=0.709 total time=   0.3s\n[CV 2/5] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=elasticnet;, score=0.602 total time=   0.5s\n[CV 5/5] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=elasticnet;, score=0.574 total time=   0.6s\n[CV 2/5] END alpha=0.0001, loss=squared_hinge, max_iter=10000, penalty=l2;, score=0.669 total time=   0.2s\n"},"10":{"name":"stdout","text":"[CV 2/5] END alpha=0.01, loss=squared_hinge, max_iter=30000, penalty=elasticnet;, score=0.733 total time=   0.4s\n[CV 4/5] END alpha=0.01, loss=squared_hinge, max_iter=30000, penalty=elasticnet;, score=0.828 total time=   0.5s\n[CV 1/5] END alpha=0.01, loss=perceptron, max_iter=1000, penalty=l2;, score=0.472 total time=   0.2s\n[CV 3/5] END alpha=0.01, loss=perceptron, max_iter=1000, penalty=l2;, score=0.600 total time=   0.1s\n[CV 5/5] END alpha=0.01, loss=perceptron, max_iter=1000, penalty=l2;, score=0.591 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=perceptron, max_iter=1000, penalty=l1;, score=0.798 total time=   0.1s\n[CV 5/5] END alpha=0.01, loss=perceptron, max_iter=1000, penalty=l1;, score=0.784 total time=   0.1s\n[CV 2/5] END alpha=0.01, loss=perceptron, max_iter=1000, penalty=elasticnet;, score=0.817 total time=   0.1s\n[CV 4/5] END alpha=0.01, loss=perceptron, max_iter=1000, penalty=elasticnet;, score=0.552 total time=   0.1s\n[CV 1/5] END alpha=0.01, loss=perceptron, max_iter=10000, penalty=l2;, score=0.656 total time=   0.2s\n[CV 3/5] END alpha=0.01, loss=perceptron, max_iter=10000, penalty=l2;, score=0.569 total time=   0.2s\n[CV 5/5] END alpha=0.01, loss=perceptron, max_iter=10000, penalty=l2;, score=0.568 total time=   0.3s\n[CV 3/5] END alpha=0.01, loss=perceptron, max_iter=10000, penalty=l1;, score=0.789 total time=   0.1s\n[CV 4/5] END alpha=0.01, loss=perceptron, max_iter=10000, penalty=l1;, score=0.765 total time=   0.1s\n[CV 1/5] END alpha=0.01, loss=perceptron, max_iter=10000, penalty=elasticnet;, score=0.850 total time=   0.2s\n[CV 3/5] END alpha=0.01, loss=perceptron, max_iter=10000, penalty=elasticnet;, score=0.731 total time=   0.1s\n[CV 5/5] END alpha=0.01, loss=perceptron, max_iter=10000, penalty=elasticnet;, score=0.851 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=perceptron, max_iter=30000, penalty=l2;, score=0.589 total time=   0.2s\n[CV 4/5] END alpha=0.01, loss=perceptron, max_iter=30000, penalty=l2;, score=0.806 total time=   0.2s\n[CV 1/5] END alpha=0.01, loss=perceptron, max_iter=30000, penalty=l1;, score=0.793 total time=   0.1s\n[CV 3/5] END alpha=0.01, loss=perceptron, max_iter=30000, penalty=l1;, score=0.791 total time=   0.2s\n[CV 5/5] END alpha=0.01, loss=perceptron, max_iter=30000, penalty=l1;, score=0.792 total time=   0.1s\n[CV 2/5] END alpha=0.01, loss=perceptron, max_iter=30000, penalty=elasticnet;, score=0.643 total time=   0.2s\n[CV 4/5] END alpha=0.01, loss=perceptron, max_iter=30000, penalty=elasticnet;, score=0.723 total time=   0.1s\n[CV 1/5] END alpha=0.1, loss=hinge, max_iter=1000, penalty=l2;, score=0.786 total time=   0.3s\n[CV 3/5] END alpha=0.1, loss=hinge, max_iter=1000, penalty=l2;, score=0.827 total time=   0.4s\n[CV 5/5] END alpha=0.1, loss=hinge, max_iter=1000, penalty=l2;, score=0.630 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=hinge, max_iter=1000, penalty=l1;, score=0.599 total time=   0.1s\n[CV 3/5] END alpha=0.1, loss=hinge, max_iter=1000, penalty=l1;, score=0.744 total time=   0.1s\n[CV 5/5] END alpha=0.1, loss=hinge, max_iter=1000, penalty=l1;, score=0.750 total time=   0.2s\n[CV 1/5] END alpha=0.1, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.733 total time=   0.2s\n[CV 3/5] END alpha=0.1, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.698 total time=   0.2s\n[CV 5/5] END alpha=0.1, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.693 total time=   0.4s\n[CV 2/5] END alpha=0.1, loss=hinge, max_iter=10000, penalty=l2;, score=0.496 total time=   0.3s\n[CV 5/5] END alpha=0.1, loss=hinge, max_iter=10000, penalty=l2;, score=0.757 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=hinge, max_iter=10000, penalty=l1;, score=0.674 total time=   0.2s\n[CV 5/5] END alpha=0.1, loss=hinge, max_iter=10000, penalty=l1;, score=0.558 total time=   0.3s\n[CV 1/5] END alpha=0.1, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.801 total time=   0.2s\n[CV 3/5] END alpha=0.1, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.749 total time=   0.2s\n[CV 5/5] END alpha=0.1, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.790 total time=   0.3s\n[CV 2/5] END alpha=0.1, loss=hinge, max_iter=30000, penalty=l2;, score=0.650 total time=   0.2s\n[CV 5/5] END alpha=0.1, loss=hinge, max_iter=30000, penalty=l2;, score=0.620 total time=   0.1s\n[CV 2/5] END alpha=0.1, loss=hinge, max_iter=30000, penalty=l1;, score=0.704 total time=   0.2s\n[CV 4/5] END alpha=0.1, loss=hinge, max_iter=30000, penalty=l1;, score=0.665 total time=   0.3s\n[CV 1/5] END alpha=0.1, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.526 total time=   0.2s\n[CV 3/5] END alpha=0.1, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.729 total time=   0.2s\n[CV 5/5] END alpha=0.1, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.637 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=log_loss, max_iter=1000, penalty=l2;, score=0.810 total time=   0.3s\n[CV 4/5] END alpha=0.1, loss=log_loss, max_iter=1000, penalty=l2;, score=0.638 total time=   0.5s\n[CV 1/5] END alpha=0.1, loss=log_loss, max_iter=1000, penalty=l1;, score=0.770 total time=   0.5s\n[CV 3/5] END alpha=0.1, loss=log_loss, max_iter=1000, penalty=l1;, score=0.762 total time=   0.3s\n[CV 5/5] END alpha=0.1, loss=log_loss, max_iter=1000, penalty=l1;, score=0.743 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=log_loss, max_iter=1000, penalty=elasticnet;, score=0.602 total time=   0.3s\n[CV 4/5] END alpha=0.1, loss=log_loss, max_iter=1000, penalty=elasticnet;, score=0.550 total time=   0.2s\n[CV 1/5] END alpha=0.1, loss=log_loss, max_iter=10000, penalty=l2;, score=0.597 total time=   0.3s\n[CV 3/5] END alpha=0.1, loss=log_loss, max_iter=10000, penalty=l2;, score=0.649 total time=   0.2s\n[CV 5/5] END alpha=0.1, loss=log_loss, max_iter=10000, penalty=l2;, score=0.581 total time=   0.3s\n[CV 1/5] END alpha=0.1, loss=log_loss, max_iter=10000, penalty=l1;, score=0.712 total time=   0.1s\n[CV 2/5] END alpha=0.1, loss=log_loss, max_iter=10000, penalty=l1;, score=0.720 total time=   0.1s\n[CV 4/5] END alpha=0.1, loss=log_loss, max_iter=10000, penalty=l1;, score=0.707 total time=   0.3s\n[CV 2/5] END alpha=0.1, loss=log_loss, max_iter=10000, penalty=elasticnet;, score=0.654 total time=   0.2s\n[CV 3/5] END alpha=0.1, loss=log_loss, max_iter=10000, penalty=elasticnet;, score=0.521 total time=   0.3s\n[CV 5/5] END alpha=0.1, loss=log_loss, max_iter=10000, penalty=elasticnet;, score=0.764 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=log_loss, max_iter=30000, penalty=l2;, score=0.521 total time=   0.2s\n[CV 3/5] END alpha=0.1, loss=log_loss, max_iter=30000, penalty=l2;, score=0.567 total time=   0.2s\n[CV 5/5] END alpha=0.1, loss=log_loss, max_iter=30000, penalty=l2;, score=0.811 total time=   0.3s\n[CV 3/5] END alpha=0.1, loss=log_loss, max_iter=30000, penalty=l1;, score=0.719 total time=   0.2s\n[CV 4/5] END alpha=0.1, loss=log_loss, max_iter=30000, penalty=l1;, score=0.775 total time=   0.3s\n[CV 2/5] END alpha=0.1, loss=log_loss, max_iter=30000, penalty=elasticnet;, score=0.489 total time=   0.2s\n[CV 4/5] END alpha=0.1, loss=log_loss, max_iter=30000, penalty=elasticnet;, score=0.787 total time=   0.2s\n[CV 1/5] END alpha=0.1, loss=modified_huber, max_iter=1000, penalty=l2;, score=0.708 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=modified_huber, max_iter=1000, penalty=l2;, score=0.573 total time=   0.3s\n[CV 5/5] END alpha=0.1, loss=modified_huber, max_iter=1000, penalty=l2;, score=0.746 total time=   0.1s\n[CV 2/5] END alpha=0.1, loss=modified_huber, max_iter=1000, penalty=l1;, score=0.788 total time=   0.1s\n[CV 3/5] END alpha=0.1, loss=modified_huber, max_iter=1000, penalty=l1;, score=0.752 total time=   0.1s\n[CV 4/5] END alpha=0.1, loss=modified_huber, max_iter=1000, penalty=l1;, score=0.747 total time=   0.1s\n[CV 1/5] END alpha=0.1, loss=modified_huber, max_iter=1000, penalty=elasticnet;, score=0.851 total time=   0.2s\n[CV 3/5] END alpha=0.1, loss=modified_huber, max_iter=1000, penalty=elasticnet;, score=0.646 total time=   0.1s\n[CV 5/5] END alpha=0.1, loss=modified_huber, max_iter=1000, penalty=elasticnet;, score=0.437 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=modified_huber, max_iter=10000, penalty=l2;, score=0.638 total time=   0.2s\n"},"11":{"name":"stdout","text":"[CV 5/5] END alpha=0.1, loss=modified_huber, max_iter=1000, penalty=l1;, score=0.756 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=modified_huber, max_iter=1000, penalty=elasticnet;, score=0.673 total time=   0.1s\n[CV 4/5] END alpha=0.1, loss=modified_huber, max_iter=1000, penalty=elasticnet;, score=0.773 total time=   0.1s\n[CV 1/5] END alpha=0.1, loss=modified_huber, max_iter=10000, penalty=l2;, score=0.573 total time=   0.2s\n[CV 3/5] END alpha=0.1, loss=modified_huber, max_iter=10000, penalty=l2;, score=0.506 total time=   0.2s\n[CV 1/5] END alpha=0.1, loss=modified_huber, max_iter=10000, penalty=l1;, score=0.771 total time=   0.5s\n[CV 4/5] END alpha=0.1, loss=modified_huber, max_iter=10000, penalty=l1;, score=0.760 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=modified_huber, max_iter=10000, penalty=elasticnet;, score=0.857 total time=   0.2s\n[CV 4/5] END alpha=0.1, loss=modified_huber, max_iter=10000, penalty=elasticnet;, score=0.825 total time=   0.2s\n[CV 1/5] END alpha=0.1, loss=modified_huber, max_iter=30000, penalty=l2;, score=0.663 total time=   0.2s\n[CV 3/5] END alpha=0.1, loss=modified_huber, max_iter=30000, penalty=l2;, score=0.568 total time=   0.2s\n[CV 5/5] END alpha=0.1, loss=modified_huber, max_iter=30000, penalty=l2;, score=0.729 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=modified_huber, max_iter=30000, penalty=l1;, score=0.783 total time=   0.3s\n[CV 4/5] END alpha=0.1, loss=modified_huber, max_iter=30000, penalty=l1;, score=0.729 total time=   0.5s\n[CV 5/5] END alpha=0.1, loss=modified_huber, max_iter=30000, penalty=l1;, score=0.782 total time=   0.6s\n[CV 1/5] END alpha=0.1, loss=modified_huber, max_iter=30000, penalty=elasticnet;, score=0.495 total time=   0.1s\n[CV 3/5] END alpha=0.1, loss=modified_huber, max_iter=30000, penalty=elasticnet;, score=0.676 total time=   0.1s\n[CV 5/5] END alpha=0.1, loss=modified_huber, max_iter=30000, penalty=elasticnet;, score=0.728 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=squared_hinge, max_iter=1000, penalty=l2;, score=0.574 total time=   0.2s\n[CV 4/5] END alpha=0.1, loss=squared_hinge, max_iter=1000, penalty=l2;, score=0.643 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=squared_hinge, max_iter=1000, penalty=l1;, score=0.771 total time=   0.4s\n[CV 4/5] END alpha=0.1, loss=squared_hinge, max_iter=1000, penalty=l1;, score=0.766 total time=   0.4s\n[CV 5/5] END alpha=0.1, loss=squared_hinge, max_iter=1000, penalty=l1;, score=0.776 total time=   0.3s\n[CV 2/5] END alpha=0.1, loss=squared_hinge, max_iter=1000, penalty=elasticnet;, score=0.613 total time=   0.3s\n[CV 4/5] END alpha=0.1, loss=squared_hinge, max_iter=1000, penalty=elasticnet;, score=0.811 total time=   0.4s\n[CV 1/5] END alpha=0.1, loss=squared_hinge, max_iter=10000, penalty=l2;, score=0.699 total time=   0.1s\n[CV 2/5] END alpha=0.1, loss=squared_hinge, max_iter=10000, penalty=l2;, score=0.503 total time=   0.2s\n[CV 3/5] END alpha=0.1, loss=squared_hinge, max_iter=10000, penalty=l2;, score=0.606 total time=   0.2s\n[CV 5/5] END alpha=0.1, loss=squared_hinge, max_iter=10000, penalty=l2;, score=0.676 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=squared_hinge, max_iter=10000, penalty=l1;, score=0.795 total time=   0.3s\n[CV 3/5] END alpha=0.1, loss=squared_hinge, max_iter=10000, penalty=l1;, score=0.694 total time=   0.3s\n[CV 5/5] END alpha=0.1, loss=squared_hinge, max_iter=10000, penalty=l1;, score=0.780 total time=   0.4s\n[CV 1/5] END alpha=0.1, loss=squared_hinge, max_iter=10000, penalty=elasticnet;, score=0.496 total time=   0.3s\n[CV 2/5] END alpha=0.1, loss=squared_hinge, max_iter=10000, penalty=elasticnet;, score=0.682 total time=   0.3s\n[CV 3/5] END alpha=0.1, loss=squared_hinge, max_iter=10000, penalty=elasticnet;, score=0.531 total time=   0.3s\n[CV 4/5] END alpha=0.1, loss=squared_hinge, max_iter=10000, penalty=elasticnet;, score=0.653 total time=   0.6s\n[CV 1/5] END alpha=0.1, loss=squared_hinge, max_iter=30000, penalty=l2;, score=0.612 total time=   0.2s\n[CV 3/5] END alpha=0.1, loss=squared_hinge, max_iter=30000, penalty=l2;, score=0.715 total time=   0.2s\n[CV 5/5] END alpha=0.1, loss=squared_hinge, max_iter=30000, penalty=l2;, score=0.545 total time=   0.1s\n[CV 2/5] END alpha=0.1, loss=squared_hinge, max_iter=30000, penalty=l1;, score=0.771 total time=   0.4s\n[CV 4/5] END alpha=0.1, loss=squared_hinge, max_iter=30000, penalty=l1;, score=0.732 total time=   0.2s\n[CV 1/5] END alpha=0.1, loss=squared_hinge, max_iter=30000, penalty=elasticnet;, score=0.568 total time=   0.3s\n[CV 3/5] END alpha=0.1, loss=squared_hinge, max_iter=30000, penalty=elasticnet;, score=0.613 total time=   0.3s\n[CV 5/5] END alpha=0.1, loss=squared_hinge, max_iter=30000, penalty=elasticnet;, score=0.812 total time=   0.4s\n[CV 2/5] END alpha=0.1, loss=perceptron, max_iter=1000, penalty=l2;, score=0.569 total time=   0.2s\n[CV 4/5] END alpha=0.1, loss=perceptron, max_iter=1000, penalty=l2;, score=0.576 total time=   0.1s\n[CV 1/5] END alpha=0.1, loss=perceptron, max_iter=1000, penalty=l1;, score=0.453 total time=   0.1s\n[CV 2/5] END alpha=0.1, loss=perceptron, max_iter=1000, penalty=l1;, score=0.711 total time=   0.3s\n[CV 5/5] END alpha=0.1, loss=perceptron, max_iter=1000, penalty=l1;, score=0.560 total time=   0.1s\n[CV 2/5] END alpha=0.1, loss=perceptron, max_iter=1000, penalty=elasticnet;, score=0.494 total time=   0.1s\n[CV 3/5] END alpha=0.1, loss=perceptron, max_iter=1000, penalty=elasticnet;, score=0.578 total time=   0.1s\n[CV 5/5] END alpha=0.1, loss=perceptron, max_iter=1000, penalty=elasticnet;, score=0.671 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=perceptron, max_iter=10000, penalty=l2;, score=0.568 total time=   0.2s\n[CV 4/5] END alpha=0.1, loss=perceptron, max_iter=10000, penalty=l2;, score=0.711 total time=   0.2s\n[CV 1/5] END alpha=0.1, loss=perceptron, max_iter=10000, penalty=l1;, score=0.719 total time=   0.3s\n[CV 3/5] END alpha=0.1, loss=perceptron, max_iter=10000, penalty=l1;, score=0.658 total time=   0.1s\n[CV 4/5] END alpha=0.1, loss=perceptron, max_iter=10000, penalty=l1;, score=0.572 total time=   0.3s\n[CV 2/5] END alpha=0.1, loss=perceptron, max_iter=10000, penalty=elasticnet;, score=0.467 total time=   0.2s\n[CV 4/5] END alpha=0.1, loss=perceptron, max_iter=10000, penalty=elasticnet;, score=0.818 total time=   0.2s\n[CV 1/5] END alpha=0.1, loss=perceptron, max_iter=30000, penalty=l2;, score=0.776 total time=   0.1s\n[CV 3/5] END alpha=0.1, loss=perceptron, max_iter=30000, penalty=l2;, score=0.760 total time=   0.2s\n[CV 5/5] END alpha=0.1, loss=perceptron, max_iter=30000, penalty=l2;, score=0.637 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=perceptron, max_iter=30000, penalty=l1;, score=0.710 total time=   0.1s\n[CV 3/5] END alpha=0.1, loss=perceptron, max_iter=30000, penalty=l1;, score=0.737 total time=   0.3s\n[CV 5/5] END alpha=0.1, loss=perceptron, max_iter=30000, penalty=l1;, score=0.479 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=perceptron, max_iter=30000, penalty=elasticnet;, score=0.483 total time=   0.2s\n[CV 4/5] END alpha=0.1, loss=perceptron, max_iter=30000, penalty=elasticnet;, score=0.677 total time=   0.2s\n[CV 1/5] END alpha=1.0, loss=hinge, max_iter=1000, penalty=l2;, score=0.696 total time=   0.2s\n[CV 3/5] END alpha=1.0, loss=hinge, max_iter=1000, penalty=l2;, score=0.568 total time=   0.2s\n[CV 5/5] END alpha=1.0, loss=hinge, max_iter=1000, penalty=l2;, score=0.644 total time=   0.1s\n[CV 2/5] END alpha=1.0, loss=hinge, max_iter=1000, penalty=l1;, score=0.577 total time=   2.7s\n[CV 1/5] END alpha=1.0, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.566 total time=   0.2s\n[CV 2/5] END alpha=1.0, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.506 total time=   0.2s\n[CV 3/5] END alpha=1.0, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.566 total time=   0.3s\n[CV 5/5] END alpha=1.0, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.517 total time=   0.1s\n[CV 2/5] END alpha=1.0, loss=hinge, max_iter=10000, penalty=l2;, score=0.581 total time=   0.2s\n[CV 4/5] END alpha=1.0, loss=hinge, max_iter=10000, penalty=l2;, score=0.653 total time=   0.2s\n[CV 1/5] END alpha=1.0, loss=hinge, max_iter=10000, penalty=l1;, score=0.540 total time=   0.2s\n"},"12":{"name":"stdout","text":"[CV 4/5] END alpha=0.1, loss=modified_huber, max_iter=10000, penalty=l2;, score=0.739 total time=   0.1s\n[CV 5/5] END alpha=0.1, loss=modified_huber, max_iter=10000, penalty=l2;, score=0.718 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=modified_huber, max_iter=10000, penalty=l1;, score=0.752 total time=   0.2s\n[CV 3/5] END alpha=0.1, loss=modified_huber, max_iter=10000, penalty=l1;, score=0.792 total time=   0.2s\n[CV 5/5] END alpha=0.1, loss=modified_huber, max_iter=10000, penalty=l1;, score=0.785 total time=   0.2s\n[CV 1/5] END alpha=0.1, loss=modified_huber, max_iter=10000, penalty=elasticnet;, score=0.773 total time=   0.1s\n[CV 3/5] END alpha=0.1, loss=modified_huber, max_iter=10000, penalty=elasticnet;, score=0.556 total time=   0.1s\n[CV 5/5] END alpha=0.1, loss=modified_huber, max_iter=10000, penalty=elasticnet;, score=0.694 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=modified_huber, max_iter=30000, penalty=l2;, score=0.807 total time=   0.2s\n[CV 4/5] END alpha=0.1, loss=modified_huber, max_iter=30000, penalty=l2;, score=0.468 total time=   0.2s\n[CV 1/5] END alpha=0.1, loss=modified_huber, max_iter=30000, penalty=l1;, score=0.789 total time=   0.1s\n[CV 3/5] END alpha=0.1, loss=modified_huber, max_iter=30000, penalty=l1;, score=0.709 total time=   1.3s\n[CV 2/5] END alpha=0.1, loss=modified_huber, max_iter=30000, penalty=elasticnet;, score=0.629 total time=   0.1s\n[CV 4/5] END alpha=0.1, loss=modified_huber, max_iter=30000, penalty=elasticnet;, score=0.793 total time=   0.1s\n[CV 1/5] END alpha=0.1, loss=squared_hinge, max_iter=1000, penalty=l2;, score=0.487 total time=   0.2s\n[CV 3/5] END alpha=0.1, loss=squared_hinge, max_iter=1000, penalty=l2;, score=0.583 total time=   0.2s\n[CV 5/5] END alpha=0.1, loss=squared_hinge, max_iter=1000, penalty=l2;, score=0.514 total time=   0.1s\n[CV 1/5] END alpha=0.1, loss=squared_hinge, max_iter=1000, penalty=l1;, score=0.750 total time=   0.3s\n[CV 3/5] END alpha=0.1, loss=squared_hinge, max_iter=1000, penalty=l1;, score=0.706 total time=   0.5s\n[CV 1/5] END alpha=0.1, loss=squared_hinge, max_iter=1000, penalty=elasticnet;, score=0.827 total time=   0.4s\n[CV 3/5] END alpha=0.1, loss=squared_hinge, max_iter=1000, penalty=elasticnet;, score=0.605 total time=   0.5s\n[CV 5/5] END alpha=0.1, loss=squared_hinge, max_iter=1000, penalty=elasticnet;, score=0.839 total time=   0.5s\n[CV 4/5] END alpha=0.1, loss=squared_hinge, max_iter=10000, penalty=l2;, score=0.752 total time=   0.3s\n[CV 1/5] END alpha=0.1, loss=squared_hinge, max_iter=10000, penalty=l1;, score=0.705 total time=   0.4s\n[CV 4/5] END alpha=0.1, loss=squared_hinge, max_iter=10000, penalty=l1;, score=0.594 total time=   1.8s\n[CV 5/5] END alpha=0.1, loss=squared_hinge, max_iter=10000, penalty=elasticnet;, score=0.582 total time=   0.3s\n[CV 2/5] END alpha=0.1, loss=squared_hinge, max_iter=30000, penalty=l2;, score=0.783 total time=   0.2s\n[CV 4/5] END alpha=0.1, loss=squared_hinge, max_iter=30000, penalty=l2;, score=0.755 total time=   0.2s\n[CV 1/5] END alpha=0.1, loss=squared_hinge, max_iter=30000, penalty=l1;, score=0.734 total time=   0.3s\n[CV 3/5] END alpha=0.1, loss=squared_hinge, max_iter=30000, penalty=l1;, score=0.749 total time=   0.3s\n[CV 5/5] END alpha=0.1, loss=squared_hinge, max_iter=30000, penalty=l1;, score=0.664 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=squared_hinge, max_iter=30000, penalty=elasticnet;, score=0.687 total time=   0.3s\n[CV 4/5] END alpha=0.1, loss=squared_hinge, max_iter=30000, penalty=elasticnet;, score=0.822 total time=   0.4s\n[CV 1/5] END alpha=0.1, loss=perceptron, max_iter=1000, penalty=l2;, score=0.579 total time=   0.3s\n[CV 3/5] END alpha=0.1, loss=perceptron, max_iter=1000, penalty=l2;, score=0.617 total time=   0.2s\n[CV 5/5] END alpha=0.1, loss=perceptron, max_iter=1000, penalty=l2;, score=0.509 total time=   0.2s\n[CV 3/5] END alpha=0.1, loss=perceptron, max_iter=1000, penalty=l1;, score=0.668 total time=   0.2s\n[CV 4/5] END alpha=0.1, loss=perceptron, max_iter=1000, penalty=l1;, score=0.731 total time=   0.2s\n[CV 1/5] END alpha=0.1, loss=perceptron, max_iter=1000, penalty=elasticnet;, score=0.675 total time=   0.2s\n[CV 4/5] END alpha=0.1, loss=perceptron, max_iter=1000, penalty=elasticnet;, score=0.765 total time=   0.1s\n[CV 1/5] END alpha=0.1, loss=perceptron, max_iter=10000, penalty=l2;, score=0.572 total time=   0.3s\n[CV 3/5] END alpha=0.1, loss=perceptron, max_iter=10000, penalty=l2;, score=0.834 total time=   0.2s\n[CV 5/5] END alpha=0.1, loss=perceptron, max_iter=10000, penalty=l2;, score=0.610 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=perceptron, max_iter=10000, penalty=l1;, score=0.720 total time=   0.3s\n[CV 5/5] END alpha=0.1, loss=perceptron, max_iter=10000, penalty=l1;, score=0.740 total time=   0.2s\n[CV 1/5] END alpha=0.1, loss=perceptron, max_iter=10000, penalty=elasticnet;, score=0.486 total time=   0.2s\n[CV 3/5] END alpha=0.1, loss=perceptron, max_iter=10000, penalty=elasticnet;, score=0.653 total time=   0.2s\n[CV 5/5] END alpha=0.1, loss=perceptron, max_iter=10000, penalty=elasticnet;, score=0.693 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=perceptron, max_iter=30000, penalty=l2;, score=0.566 total time=   0.2s\n[CV 4/5] END alpha=0.1, loss=perceptron, max_iter=30000, penalty=l2;, score=0.575 total time=   0.2s\n[CV 1/5] END alpha=0.1, loss=perceptron, max_iter=30000, penalty=l1;, score=0.716 total time=   0.5s\n[CV 4/5] END alpha=0.1, loss=perceptron, max_iter=30000, penalty=l1;, score=0.670 total time=   0.3s\n[CV 1/5] END alpha=0.1, loss=perceptron, max_iter=30000, penalty=elasticnet;, score=0.777 total time=   0.1s\n[CV 3/5] END alpha=0.1, loss=perceptron, max_iter=30000, penalty=elasticnet;, score=0.757 total time=   0.2s\n[CV 5/5] END alpha=0.1, loss=perceptron, max_iter=30000, penalty=elasticnet;, score=0.772 total time=   0.2s\n[CV 2/5] END alpha=1.0, loss=hinge, max_iter=1000, penalty=l2;, score=0.766 total time=   0.2s\n[CV 4/5] END alpha=1.0, loss=hinge, max_iter=1000, penalty=l2;, score=0.655 total time=   0.1s\n[CV 1/5] END alpha=1.0, loss=hinge, max_iter=1000, penalty=l1;, score=0.566 total time=   0.2s\n[CV 3/5] END alpha=1.0, loss=hinge, max_iter=1000, penalty=l1;, score=0.555 total time=   0.4s\n[CV 4/5] END alpha=1.0, loss=hinge, max_iter=1000, penalty=l1;, score=0.565 total time=   0.2s\n[CV 5/5] END alpha=1.0, loss=hinge, max_iter=1000, penalty=l1;, score=0.560 total time=   2.5s\n[CV 4/5] END alpha=1.0, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.567 total time=   0.3s\n[CV 1/5] END alpha=1.0, loss=hinge, max_iter=10000, penalty=l2;, score=0.785 total time=   0.2s\n[CV 3/5] END alpha=1.0, loss=hinge, max_iter=10000, penalty=l2;, score=0.588 total time=   0.1s\n[CV 5/5] END alpha=1.0, loss=hinge, max_iter=10000, penalty=l2;, score=0.692 total time=   0.2s\n[CV 2/5] END alpha=1.0, loss=hinge, max_iter=10000, penalty=l1;, score=0.461 total time=   0.2s\n[CV 4/5] END alpha=1.0, loss=hinge, max_iter=10000, penalty=l1;, score=0.480 total time=   0.3s\n[CV 2/5] END alpha=1.0, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.636 total time=   0.3s\n[CV 4/5] END alpha=1.0, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.461 total time=   0.2s\n[CV 5/5] END alpha=1.0, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.503 total time=   0.3s\n[CV 3/5] END alpha=1.0, loss=hinge, max_iter=30000, penalty=l2;, score=0.674 total time=   0.2s\n[CV 5/5] END alpha=1.0, loss=hinge, max_iter=30000, penalty=l2;, score=0.565 total time=   0.1s\n[CV 1/5] END alpha=1.0, loss=hinge, max_iter=30000, penalty=l1;, score=0.566 total time=   0.2s\n[CV 3/5] END alpha=1.0, loss=hinge, max_iter=30000, penalty=l1;, score=0.566 total time=   0.4s\n[CV 4/5] END alpha=1.0, loss=hinge, max_iter=30000, penalty=l1;, score=0.577 total time=   0.3s\n[CV 5/5] END alpha=1.0, loss=hinge, max_iter=30000, penalty=l1;, score=0.493 total time=   0.2s\n[CV 1/5] END alpha=1.0, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.566 total time=   0.2s\n[CV 2/5] END alpha=1.0, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.566 total time=   0.3s\n[CV 3/5] END alpha=1.0, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.566 total time=   0.2s\n"},"13":{"more_output":true},"2":{"name":"stdout","text":"[CV 1/5] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=l2;, score=0.571 total time=   0.3s\n[CV 3/5] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=l2;, score=0.630 total time=   0.2s\n[CV 1/5] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=l1;, score=0.758 total time=   0.3s\n[CV 3/5] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=l1;, score=0.717 total time=   0.6s\n[CV 1/5] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.632 total time=   0.4s\n[CV 3/5] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.572 total time=   0.3s\n[CV 5/5] END alpha=0.0001, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.818 total time=   0.5s\n[CV 2/5] END alpha=0.0001, loss=hinge, max_iter=10000, penalty=l2;, score=0.709 total time=   0.2s\n[CV 4/5] END alpha=0.0001, loss=hinge, max_iter=10000, penalty=l2;, score=0.800 total time=   0.2s\n[CV 5/5] END alpha=0.0001, loss=hinge, max_iter=10000, penalty=l2;, score=0.670 total time=   0.1s\n[CV 2/5] END alpha=0.0001, loss=hinge, max_iter=10000, penalty=l1;, score=0.805 total time=   0.2s\n[CV 4/5] END alpha=0.0001, loss=hinge, max_iter=10000, penalty=l1;, score=0.729 total time=   0.3s\n[CV 5/5] END alpha=0.0001, loss=hinge, max_iter=10000, penalty=l1;, score=0.745 total time=   0.2s\n[CV 2/5] END alpha=0.0001, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.847 total time=   0.5s\n[CV 5/5] END alpha=0.0001, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.494 total time=   0.3s\n[CV 1/5] END alpha=0.0001, loss=hinge, max_iter=30000, penalty=l2;, score=0.636 total time=   0.2s\n[CV 2/5] END alpha=0.0001, loss=hinge, max_iter=30000, penalty=l2;, score=0.574 total time=   0.3s\n[CV 5/5] END alpha=0.0001, loss=hinge, max_iter=30000, penalty=l2;, score=0.676 total time=   0.2s\n[CV 2/5] END alpha=0.0001, loss=hinge, max_iter=30000, penalty=l1;, score=0.748 total time=   0.3s\n[CV 4/5] END alpha=0.0001, loss=hinge, max_iter=30000, penalty=l1;, score=0.778 total time=   0.3s\n[CV 1/5] END alpha=0.0001, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.569 total time=   0.3s\n[CV 2/5] END alpha=0.0001, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.785 total time=   0.5s\n[CV 4/5] END alpha=0.0001, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.726 total time=   0.5s\n[CV 1/5] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2;, score=0.600 total time=   0.2s\n[CV 3/5] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l2;, score=0.569 total time=   0.3s\n[CV 1/5] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1;, score=0.614 total time=   0.3s\n[CV 3/5] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1;, score=0.795 total time=   0.4s\n[CV 5/5] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=l1;, score=0.687 total time=   0.6s\n[CV 2/5] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet;, score=0.586 total time=   0.3s\n[CV 4/5] END alpha=0.0001, loss=log_loss, max_iter=1000, penalty=elasticnet;, score=0.812 total time=   0.4s\n[CV 1/5] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2;, score=0.678 total time=   0.2s\n[CV 2/5] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2;, score=0.503 total time=   0.3s\n[CV 4/5] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l2;, score=0.523 total time=   0.2s\n[CV 1/5] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1;, score=0.792 total time=   0.3s\n[CV 3/5] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1;, score=0.782 total time=   0.3s\n[CV 5/5] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=l1;, score=0.803 total time=   0.4s\n[CV 2/5] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet;, score=0.461 total time=   0.4s\n[CV 4/5] END alpha=0.0001, loss=log_loss, max_iter=10000, penalty=elasticnet;, score=0.591 total time=   0.5s\n[CV 1/5] END alpha=0.0001, loss=log_loss, max_iter=30000, penalty=l2;, score=0.536 total time=   0.2s\n[CV 3/5] END alpha=0.0001, loss=log_loss, max_iter=30000, penalty=l2;, score=0.620 total time=   0.2s\n[CV 4/5] END alpha=0.0001, loss=log_loss, max_iter=30000, penalty=l2;, score=0.643 total time=   0.2s\n[CV 1/5] END alpha=0.0001, loss=log_loss, max_iter=30000, penalty=l1;, score=0.798 total time=   0.5s\n[CV 4/5] END alpha=0.0001, loss=log_loss, max_iter=30000, penalty=l1;, score=0.714 total time=   0.4s\n[CV 5/5] END alpha=0.0001, loss=log_loss, max_iter=30000, penalty=l1;, score=0.801 total time=   0.4s\n[CV 2/5] END alpha=0.0001, loss=log_loss, max_iter=30000, penalty=elasticnet;, score=0.569 total time=   0.5s\n[CV 5/5] END alpha=0.0001, loss=log_loss, max_iter=30000, penalty=elasticnet;, score=0.533 total time=   0.9s\n[CV 4/5] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=l2;, score=0.566 total time=   0.1s\n[CV 1/5] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=l1;, score=0.771 total time=   0.3s\n[CV 3/5] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=l1;, score=0.792 total time=   0.4s\n[CV 5/5] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=l1;, score=0.781 total time=   0.4s\n[CV 2/5] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=elasticnet;, score=0.770 total time=   0.3s\n[CV 4/5] END alpha=0.0001, loss=modified_huber, max_iter=1000, penalty=elasticnet;, score=0.639 total time=   0.4s\n[CV 1/5] END alpha=0.0001, loss=modified_huber, max_iter=10000, penalty=l2;, score=0.819 total time=   0.2s\n[CV 3/5] END alpha=0.0001, loss=modified_huber, max_iter=10000, penalty=l2;, score=0.771 total time=   0.1s\n[CV 5/5] END alpha=0.0001, loss=modified_huber, max_iter=10000, penalty=l2;, score=0.501 total time=   0.2s\n[CV 2/5] END alpha=0.0001, loss=modified_huber, max_iter=10000, penalty=l1;, score=0.808 total time=   0.4s\n[CV 5/5] END alpha=0.0001, loss=modified_huber, max_iter=10000, penalty=l1;, score=0.786 total time=   0.4s\n[CV 2/5] END alpha=0.0001, loss=modified_huber, max_iter=10000, penalty=elasticnet;, score=0.673 total time=   0.3s\n[CV 4/5] END alpha=0.0001, loss=modified_huber, max_iter=10000, penalty=elasticnet;, score=0.726 total time=   0.2s\n[CV 1/5] END alpha=0.0001, loss=modified_huber, max_iter=30000, penalty=l2;, score=0.653 total time=   0.1s\n[CV 2/5] END alpha=0.0001, loss=modified_huber, max_iter=30000, penalty=l2;, score=0.576 total time=   0.2s\n[CV 4/5] END alpha=0.0001, loss=modified_huber, max_iter=30000, penalty=l2;, score=0.743 total time=   0.2s\n[CV 1/5] END alpha=0.0001, loss=modified_huber, max_iter=30000, penalty=l1;, score=0.747 total time=   0.5s\n[CV 3/5] END alpha=0.0001, loss=modified_huber, max_iter=30000, penalty=l1;, score=0.796 total time=   0.4s\n[CV 1/5] END alpha=0.0001, loss=modified_huber, max_iter=30000, penalty=elasticnet;, score=0.496 total time=   0.4s\n[CV 3/5] END alpha=0.0001, loss=modified_huber, max_iter=30000, penalty=elasticnet;, score=0.708 total time=   0.4s\n[CV 4/5] END alpha=0.0001, loss=modified_huber, max_iter=30000, penalty=elasticnet;, score=0.813 total time=   0.4s\n[CV 2/5] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=l2;, score=0.531 total time=   0.2s\n[CV 4/5] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=l2;, score=0.505 total time=   0.3s\n[CV 1/5] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=l1;, score=0.720 total time=   0.3s\n[CV 3/5] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=l1;, score=0.674 total time=   0.3s\n[CV 5/5] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=l1;, score=0.684 total time=   0.4s\n[CV 3/5] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=elasticnet;, score=0.592 total time=   0.5s\n[CV 4/5] END alpha=0.0001, loss=squared_hinge, max_iter=1000, penalty=elasticnet;, score=0.501 total time=   0.5s\n[CV 1/5] END alpha=0.0001, loss=squared_hinge, max_iter=10000, penalty=l2;, score=0.787 total time=   0.2s\n[CV 3/5] END alpha=0.0001, loss=squared_hinge, max_iter=10000, penalty=l2;, score=0.452 total time=   0.2s\n[CV 5/5] END alpha=0.0001, loss=squared_hinge, max_iter=10000, penalty=l2;, score=0.554 total time=   0.2s\n"},"3":{"name":"stdout","text":"[CV 4/5] END alpha=0.0001, loss=squared_hinge, max_iter=10000, penalty=l2;, score=0.611 total time=   0.1s\n[CV 1/5] END alpha=0.0001, loss=squared_hinge, max_iter=10000, penalty=l1;, score=0.734 total time=   0.4s\n[CV 3/5] END alpha=0.0001, loss=squared_hinge, max_iter=10000, penalty=l1;, score=0.783 total time=   0.3s\n[CV 5/5] END alpha=0.0001, loss=squared_hinge, max_iter=10000, penalty=l1;, score=0.777 total time=   0.3s\n[CV 2/5] END alpha=0.0001, loss=squared_hinge, max_iter=10000, penalty=elasticnet;, score=0.468 total time=   0.4s\n[CV 4/5] END alpha=0.0001, loss=squared_hinge, max_iter=10000, penalty=elasticnet;, score=0.792 total time=   0.4s\n[CV 1/5] END alpha=0.0001, loss=squared_hinge, max_iter=30000, penalty=l2;, score=0.717 total time=   0.1s\n[CV 3/5] END alpha=0.0001, loss=squared_hinge, max_iter=30000, penalty=l2;, score=0.588 total time=   0.1s\n[CV 4/5] END alpha=0.0001, loss=squared_hinge, max_iter=30000, penalty=l2;, score=0.656 total time=   0.2s\n[CV 1/5] END alpha=0.0001, loss=squared_hinge, max_iter=30000, penalty=l1;, score=0.785 total time=   0.4s\n[CV 4/5] END alpha=0.0001, loss=squared_hinge, max_iter=30000, penalty=l1;, score=0.767 total time=   0.3s\n[CV 5/5] END alpha=0.0001, loss=squared_hinge, max_iter=30000, penalty=l1;, score=0.727 total time=   0.5s\n[CV 3/5] END alpha=0.0001, loss=squared_hinge, max_iter=30000, penalty=elasticnet;, score=0.741 total time=   0.4s\n[CV 4/5] END alpha=0.0001, loss=squared_hinge, max_iter=30000, penalty=elasticnet;, score=0.523 total time=   0.3s\n[CV 1/5] END alpha=0.0001, loss=perceptron, max_iter=1000, penalty=l2;, score=0.473 total time=   0.2s\n[CV 3/5] END alpha=0.0001, loss=perceptron, max_iter=1000, penalty=l2;, score=0.568 total time=   0.2s\n[CV 5/5] END alpha=0.0001, loss=perceptron, max_iter=1000, penalty=l2;, score=0.612 total time=   0.2s\n[CV 2/5] END alpha=0.0001, loss=perceptron, max_iter=1000, penalty=l1;, score=0.749 total time=   0.6s\n[CV 4/5] END alpha=0.0001, loss=perceptron, max_iter=1000, penalty=l1;, score=0.782 total time=   0.4s\n[CV 1/5] END alpha=0.0001, loss=perceptron, max_iter=1000, penalty=elasticnet;, score=0.612 total time=   0.3s\n[CV 3/5] END alpha=0.0001, loss=perceptron, max_iter=1000, penalty=elasticnet;, score=0.660 total time=   0.4s\n[CV 5/5] END alpha=0.0001, loss=perceptron, max_iter=1000, penalty=elasticnet;, score=0.504 total time=   0.5s\n[CV 3/5] END alpha=0.0001, loss=perceptron, max_iter=10000, penalty=l2;, score=0.530 total time=   0.3s\n[CV 5/5] END alpha=0.0001, loss=perceptron, max_iter=10000, penalty=l2;, score=0.664 total time=   0.2s\n[CV 2/5] END alpha=0.0001, loss=perceptron, max_iter=10000, penalty=l1;, score=0.718 total time=   0.5s\n[CV 4/5] END alpha=0.0001, loss=perceptron, max_iter=10000, penalty=l1;, score=0.716 total time=   0.3s\n[CV 1/5] END alpha=0.0001, loss=perceptron, max_iter=10000, penalty=elasticnet;, score=0.806 total time=   0.3s\n[CV 3/5] END alpha=0.0001, loss=perceptron, max_iter=10000, penalty=elasticnet;, score=0.575 total time=   0.2s\n[CV 5/5] END alpha=0.0001, loss=perceptron, max_iter=10000, penalty=elasticnet;, score=0.568 total time=   0.3s\n[CV 2/5] END alpha=0.0001, loss=perceptron, max_iter=30000, penalty=l2;, score=0.571 total time=   0.1s\n[CV 3/5] END alpha=0.0001, loss=perceptron, max_iter=30000, penalty=l2;, score=0.521 total time=   0.2s\n[CV 5/5] END alpha=0.0001, loss=perceptron, max_iter=30000, penalty=l2;, score=0.809 total time=   0.2s\n[CV 2/5] END alpha=0.0001, loss=perceptron, max_iter=30000, penalty=l1;, score=0.763 total time=   0.4s\n[CV 4/5] END alpha=0.0001, loss=perceptron, max_iter=30000, penalty=l1;, score=0.780 total time=   0.3s\n[CV 1/5] END alpha=0.0001, loss=perceptron, max_iter=30000, penalty=elasticnet;, score=0.534 total time=   0.3s\n[CV 3/5] END alpha=0.0001, loss=perceptron, max_iter=30000, penalty=elasticnet;, score=0.767 total time=   0.6s\n[CV 5/5] END alpha=0.0001, loss=perceptron, max_iter=30000, penalty=elasticnet;, score=0.565 total time=   0.5s\n[CV 3/5] END alpha=0.001, loss=hinge, max_iter=1000, penalty=l2;, score=0.472 total time=   0.3s\n[CV 1/5] END alpha=0.001, loss=hinge, max_iter=1000, penalty=l1;, score=0.795 total time=   0.2s\n[CV 3/5] END alpha=0.001, loss=hinge, max_iter=1000, penalty=l1;, score=0.795 total time=   0.2s\n[CV 5/5] END alpha=0.001, loss=hinge, max_iter=1000, penalty=l1;, score=0.794 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.865 total time=   0.4s\n[CV 4/5] END alpha=0.001, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.501 total time=   0.3s\n[CV 1/5] END alpha=0.001, loss=hinge, max_iter=10000, penalty=l2;, score=0.596 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=hinge, max_iter=10000, penalty=l2;, score=0.763 total time=   0.1s\n[CV 3/5] END alpha=0.001, loss=hinge, max_iter=10000, penalty=l2;, score=0.568 total time=   0.1s\n[CV 5/5] END alpha=0.001, loss=hinge, max_iter=10000, penalty=l2;, score=0.749 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=hinge, max_iter=10000, penalty=l1;, score=0.693 total time=   0.2s\n[CV 5/5] END alpha=0.001, loss=hinge, max_iter=10000, penalty=l1;, score=0.804 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.870 total time=   0.5s\n[CV 4/5] END alpha=0.001, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.856 total time=   0.4s\n[CV 1/5] END alpha=0.001, loss=hinge, max_iter=30000, penalty=l2;, score=0.631 total time=   0.1s\n[CV 2/5] END alpha=0.001, loss=hinge, max_iter=30000, penalty=l2;, score=0.524 total time=   0.2s\n[CV 4/5] END alpha=0.001, loss=hinge, max_iter=30000, penalty=l2;, score=0.734 total time=   0.3s\n[CV 1/5] END alpha=0.001, loss=hinge, max_iter=30000, penalty=l1;, score=0.794 total time=   0.3s\n[CV 3/5] END alpha=0.001, loss=hinge, max_iter=30000, penalty=l1;, score=0.795 total time=   0.2s\n[CV 5/5] END alpha=0.001, loss=hinge, max_iter=30000, penalty=l1;, score=0.805 total time=   0.3s\n[CV 2/5] END alpha=0.001, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.559 total time=   0.2s\n[CV 4/5] END alpha=0.001, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.726 total time=   0.2s\n[CV 1/5] END alpha=0.001, loss=log_loss, max_iter=1000, penalty=l2;, score=0.617 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=log_loss, max_iter=1000, penalty=l2;, score=0.489 total time=   0.2s\n[CV 4/5] END alpha=0.001, loss=log_loss, max_iter=1000, penalty=l2;, score=0.805 total time=   0.2s\n[CV 1/5] END alpha=0.001, loss=log_loss, max_iter=1000, penalty=l1;, score=0.786 total time=   0.5s\n[CV 3/5] END alpha=0.001, loss=log_loss, max_iter=1000, penalty=l1;, score=0.792 total time=   0.4s\n[CV 5/5] END alpha=0.001, loss=log_loss, max_iter=1000, penalty=l1;, score=0.650 total time=   0.2s\n[CV 1/5] END alpha=0.001, loss=log_loss, max_iter=1000, penalty=elasticnet;, score=0.838 total time=   0.3s\n[CV 3/5] END alpha=0.001, loss=log_loss, max_iter=1000, penalty=elasticnet;, score=0.643 total time=   0.3s\n[CV 5/5] END alpha=0.001, loss=log_loss, max_iter=1000, penalty=elasticnet;, score=0.792 total time=   0.3s\n[CV 2/5] END alpha=0.001, loss=log_loss, max_iter=10000, penalty=l2;, score=0.842 total time=   0.4s\n[CV 5/5] END alpha=0.001, loss=log_loss, max_iter=10000, penalty=l2;, score=0.755 total time=   0.3s\n[CV 2/5] END alpha=0.001, loss=log_loss, max_iter=10000, penalty=l1;, score=0.818 total time=   0.5s\n[CV 3/5] END alpha=0.001, loss=log_loss, max_iter=10000, penalty=l1;, score=0.695 total time=   0.2s\n[CV 1/5] END alpha=0.001, loss=log_loss, max_iter=10000, penalty=elasticnet;, score=0.856 total time=   0.5s\n[CV 4/5] END alpha=0.001, loss=log_loss, max_iter=10000, penalty=elasticnet;, score=0.844 total time=   0.4s\n[CV 1/5] END alpha=0.001, loss=log_loss, max_iter=30000, penalty=l2;, score=0.700 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=log_loss, max_iter=30000, penalty=l2;, score=0.482 total time=   0.2s\n[CV 4/5] END alpha=0.001, loss=log_loss, max_iter=30000, penalty=l2;, score=0.569 total time=   0.2s\n[CV 5/5] END alpha=0.001, loss=log_loss, max_iter=30000, penalty=l2;, score=0.623 total time=   0.2s\n"},"4":{"name":"stdout","text":"[CV 2/5] END alpha=0.0001, loss=squared_hinge, max_iter=10000, penalty=l1;, score=0.745 total time=   0.4s\n[CV 4/5] END alpha=0.0001, loss=squared_hinge, max_iter=10000, penalty=l1;, score=0.779 total time=   0.3s\n[CV 1/5] END alpha=0.0001, loss=squared_hinge, max_iter=10000, penalty=elasticnet;, score=0.597 total time=   0.4s\n[CV 3/5] END alpha=0.0001, loss=squared_hinge, max_iter=10000, penalty=elasticnet;, score=0.795 total time=   0.2s\n[CV 5/5] END alpha=0.0001, loss=squared_hinge, max_iter=10000, penalty=elasticnet;, score=0.590 total time=   0.4s\n[CV 2/5] END alpha=0.0001, loss=squared_hinge, max_iter=30000, penalty=l2;, score=0.754 total time=   0.2s\n[CV 5/5] END alpha=0.0001, loss=squared_hinge, max_iter=30000, penalty=l2;, score=0.732 total time=   0.2s\n[CV 2/5] END alpha=0.0001, loss=squared_hinge, max_iter=30000, penalty=l1;, score=0.700 total time=   0.3s\n[CV 3/5] END alpha=0.0001, loss=squared_hinge, max_iter=30000, penalty=l1;, score=0.795 total time=   0.3s\n[CV 1/5] END alpha=0.0001, loss=squared_hinge, max_iter=30000, penalty=elasticnet;, score=0.608 total time=   0.4s\n[CV 2/5] END alpha=0.0001, loss=squared_hinge, max_iter=30000, penalty=elasticnet;, score=0.801 total time=   0.4s\n[CV 5/5] END alpha=0.0001, loss=squared_hinge, max_iter=30000, penalty=elasticnet;, score=0.491 total time=   0.4s\n[CV 2/5] END alpha=0.0001, loss=perceptron, max_iter=1000, penalty=l2;, score=0.587 total time=   0.2s\n[CV 4/5] END alpha=0.0001, loss=perceptron, max_iter=1000, penalty=l2;, score=0.585 total time=   0.1s\n[CV 1/5] END alpha=0.0001, loss=perceptron, max_iter=1000, penalty=l1;, score=0.720 total time=   0.5s\n[CV 3/5] END alpha=0.0001, loss=perceptron, max_iter=1000, penalty=l1;, score=0.796 total time=   0.4s\n[CV 5/5] END alpha=0.0001, loss=perceptron, max_iter=1000, penalty=l1;, score=0.800 total time=   0.5s\n[CV 2/5] END alpha=0.0001, loss=perceptron, max_iter=1000, penalty=elasticnet;, score=0.820 total time=   0.5s\n[CV 4/5] END alpha=0.0001, loss=perceptron, max_iter=1000, penalty=elasticnet;, score=0.780 total time=   0.4s\n[CV 1/5] END alpha=0.0001, loss=perceptron, max_iter=10000, penalty=l2;, score=0.757 total time=   0.2s\n[CV 2/5] END alpha=0.0001, loss=perceptron, max_iter=10000, penalty=l2;, score=0.460 total time=   0.3s\n[CV 4/5] END alpha=0.0001, loss=perceptron, max_iter=10000, penalty=l2;, score=0.501 total time=   0.1s\n[CV 1/5] END alpha=0.0001, loss=perceptron, max_iter=10000, penalty=l1;, score=0.796 total time=   0.3s\n[CV 3/5] END alpha=0.0001, loss=perceptron, max_iter=10000, penalty=l1;, score=0.799 total time=   0.4s\n[CV 5/5] END alpha=0.0001, loss=perceptron, max_iter=10000, penalty=l1;, score=0.796 total time=   0.3s\n[CV 2/5] END alpha=0.0001, loss=perceptron, max_iter=10000, penalty=elasticnet;, score=0.740 total time=   0.3s\n[CV 4/5] END alpha=0.0001, loss=perceptron, max_iter=10000, penalty=elasticnet;, score=0.613 total time=   0.3s\n[CV 1/5] END alpha=0.0001, loss=perceptron, max_iter=30000, penalty=l2;, score=0.547 total time=   0.2s\n[CV 4/5] END alpha=0.0001, loss=perceptron, max_iter=30000, penalty=l2;, score=0.693 total time=   0.2s\n[CV 1/5] END alpha=0.0001, loss=perceptron, max_iter=30000, penalty=l1;, score=0.796 total time=   0.4s\n[CV 3/5] END alpha=0.0001, loss=perceptron, max_iter=30000, penalty=l1;, score=0.774 total time=   0.3s\n[CV 5/5] END alpha=0.0001, loss=perceptron, max_iter=30000, penalty=l1;, score=0.799 total time=   0.5s\n[CV 2/5] END alpha=0.0001, loss=perceptron, max_iter=30000, penalty=elasticnet;, score=0.499 total time=   0.5s\n[CV 4/5] END alpha=0.0001, loss=perceptron, max_iter=30000, penalty=elasticnet;, score=0.681 total time=   0.4s\n[CV 1/5] END alpha=0.001, loss=hinge, max_iter=1000, penalty=l2;, score=0.577 total time=   0.1s\n[CV 2/5] END alpha=0.001, loss=hinge, max_iter=1000, penalty=l2;, score=0.566 total time=   0.1s\n[CV 4/5] END alpha=0.001, loss=hinge, max_iter=1000, penalty=l2;, score=0.749 total time=   0.2s\n[CV 5/5] END alpha=0.001, loss=hinge, max_iter=1000, penalty=l2;, score=0.784 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=hinge, max_iter=1000, penalty=l1;, score=0.805 total time=   0.2s\n[CV 4/5] END alpha=0.001, loss=hinge, max_iter=1000, penalty=l1;, score=0.779 total time=   0.2s\n[CV 1/5] END alpha=0.001, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.855 total time=   0.3s\n[CV 3/5] END alpha=0.001, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.753 total time=   0.5s\n[CV 5/5] END alpha=0.001, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.852 total time=   0.3s\n[CV 4/5] END alpha=0.001, loss=hinge, max_iter=10000, penalty=l2;, score=0.577 total time=   0.2s\n[CV 1/5] END alpha=0.001, loss=hinge, max_iter=10000, penalty=l1;, score=0.778 total time=   0.2s\n[CV 3/5] END alpha=0.001, loss=hinge, max_iter=10000, penalty=l1;, score=0.790 total time=   0.2s\n[CV 4/5] END alpha=0.001, loss=hinge, max_iter=10000, penalty=l1;, score=0.775 total time=   0.2s\n[CV 1/5] END alpha=0.001, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.858 total time=   0.5s\n[CV 3/5] END alpha=0.001, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.724 total time=   0.3s\n[CV 5/5] END alpha=0.001, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.847 total time=   0.4s\n[CV 3/5] END alpha=0.001, loss=hinge, max_iter=30000, penalty=l2;, score=0.823 total time=   0.3s\n[CV 5/5] END alpha=0.001, loss=hinge, max_iter=30000, penalty=l2;, score=0.617 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=hinge, max_iter=30000, penalty=l1;, score=0.820 total time=   0.3s\n[CV 4/5] END alpha=0.001, loss=hinge, max_iter=30000, penalty=l1;, score=0.779 total time=   0.2s\n[CV 1/5] END alpha=0.001, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.851 total time=   0.4s\n[CV 3/5] END alpha=0.001, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.752 total time=   0.2s\n[CV 5/5] END alpha=0.001, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.854 total time=   0.3s\n[CV 3/5] END alpha=0.001, loss=log_loss, max_iter=1000, penalty=l2;, score=0.479 total time=   0.2s\n[CV 5/5] END alpha=0.001, loss=log_loss, max_iter=1000, penalty=l2;, score=0.565 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=log_loss, max_iter=1000, penalty=l1;, score=0.815 total time=   0.7s\n[CV 4/5] END alpha=0.001, loss=log_loss, max_iter=1000, penalty=l1;, score=0.784 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=log_loss, max_iter=1000, penalty=elasticnet;, score=0.859 total time=   0.4s\n[CV 4/5] END alpha=0.001, loss=log_loss, max_iter=1000, penalty=elasticnet;, score=0.843 total time=   0.3s\n[CV 1/5] END alpha=0.001, loss=log_loss, max_iter=10000, penalty=l2;, score=0.570 total time=   0.2s\n[CV 3/5] END alpha=0.001, loss=log_loss, max_iter=10000, penalty=l2;, score=0.590 total time=   0.3s\n[CV 4/5] END alpha=0.001, loss=log_loss, max_iter=10000, penalty=l2;, score=0.817 total time=   0.2s\n[CV 1/5] END alpha=0.001, loss=log_loss, max_iter=10000, penalty=l1;, score=0.802 total time=   0.5s\n[CV 4/5] END alpha=0.001, loss=log_loss, max_iter=10000, penalty=l1;, score=0.663 total time=   0.2s\n[CV 5/5] END alpha=0.001, loss=log_loss, max_iter=10000, penalty=l1;, score=0.797 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=log_loss, max_iter=10000, penalty=elasticnet;, score=0.812 total time=   0.2s\n[CV 3/5] END alpha=0.001, loss=log_loss, max_iter=10000, penalty=elasticnet;, score=0.851 total time=   0.4s\n[CV 5/5] END alpha=0.001, loss=log_loss, max_iter=10000, penalty=elasticnet;, score=0.855 total time=   0.5s\n[CV 3/5] END alpha=0.001, loss=log_loss, max_iter=30000, penalty=l2;, score=0.740 total time=   0.3s\n[CV 1/5] END alpha=0.001, loss=log_loss, max_iter=30000, penalty=l1;, score=0.603 total time=   0.2s\n[CV 3/5] END alpha=0.001, loss=log_loss, max_iter=30000, penalty=l1;, score=0.799 total time=   0.3s\n[CV 5/5] END alpha=0.001, loss=log_loss, max_iter=30000, penalty=l1;, score=0.792 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=log_loss, max_iter=30000, penalty=elasticnet;, score=0.870 total time=   0.5s\n[CV 4/5] END alpha=0.001, loss=log_loss, max_iter=30000, penalty=elasticnet;, score=0.847 total time=   0.4s\n"},"5":{"name":"stdout","text":"[CV 2/5] END alpha=0.001, loss=log_loss, max_iter=30000, penalty=l1;, score=0.817 total time=   0.3s\n[CV 4/5] END alpha=0.001, loss=log_loss, max_iter=30000, penalty=l1;, score=0.794 total time=   0.3s\n[CV 1/5] END alpha=0.001, loss=log_loss, max_iter=30000, penalty=elasticnet;, score=0.667 total time=   0.3s\n[CV 3/5] END alpha=0.001, loss=log_loss, max_iter=30000, penalty=elasticnet;, score=0.849 total time=   0.4s\n[CV 5/5] END alpha=0.001, loss=log_loss, max_iter=30000, penalty=elasticnet;, score=0.854 total time=   0.5s\n[CV 2/5] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=l2;, score=0.837 total time=   0.2s\n[CV 5/5] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=l2;, score=0.803 total time=   0.2s\n[CV 1/5] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=l1;, score=0.785 total time=   0.3s\n[CV 3/5] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=l1;, score=0.788 total time=   0.2s\n[CV 5/5] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=l1;, score=0.786 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=elasticnet;, score=0.841 total time=   0.4s\n[CV 4/5] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=elasticnet;, score=0.589 total time=   0.5s\n[CV 1/5] END alpha=0.001, loss=modified_huber, max_iter=10000, penalty=l2;, score=0.794 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=modified_huber, max_iter=10000, penalty=l2;, score=0.686 total time=   0.1s\n[CV 4/5] END alpha=0.001, loss=modified_huber, max_iter=10000, penalty=l2;, score=0.594 total time=   0.2s\n[CV 1/5] END alpha=0.001, loss=modified_huber, max_iter=10000, penalty=l1;, score=0.711 total time=   0.2s\n[CV 3/5] END alpha=0.001, loss=modified_huber, max_iter=10000, penalty=l1;, score=0.789 total time=   0.2s\n[CV 4/5] END alpha=0.001, loss=modified_huber, max_iter=10000, penalty=l1;, score=0.778 total time=   0.3s\n[CV 2/5] END alpha=0.001, loss=modified_huber, max_iter=10000, penalty=elasticnet;, score=0.752 total time=   0.3s\n[CV 3/5] END alpha=0.001, loss=modified_huber, max_iter=10000, penalty=elasticnet;, score=0.684 total time=   0.6s\n[CV 5/5] END alpha=0.001, loss=modified_huber, max_iter=10000, penalty=elasticnet;, score=0.807 total time=   0.3s\n[CV 2/5] END alpha=0.001, loss=modified_huber, max_iter=30000, penalty=l2;, score=0.485 total time=   0.2s\n[CV 4/5] END alpha=0.001, loss=modified_huber, max_iter=30000, penalty=l2;, score=0.801 total time=   0.2s\n[CV 1/5] END alpha=0.001, loss=modified_huber, max_iter=30000, penalty=l1;, score=0.785 total time=   0.3s\n[CV 3/5] END alpha=0.001, loss=modified_huber, max_iter=30000, penalty=l1;, score=0.785 total time=   0.3s\n[CV 5/5] END alpha=0.001, loss=modified_huber, max_iter=30000, penalty=l1;, score=0.777 total time=   0.3s\n[CV 2/5] END alpha=0.001, loss=modified_huber, max_iter=30000, penalty=elasticnet;, score=0.808 total time=   0.3s\n[CV 4/5] END alpha=0.001, loss=modified_huber, max_iter=30000, penalty=elasticnet;, score=0.834 total time=   0.4s\n[CV 1/5] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=l2;, score=0.762 total time=   0.1s\n[CV 2/5] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=l2;, score=0.716 total time=   0.2s\n[CV 3/5] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=l2;, score=0.693 total time=   0.2s\n[CV 1/5] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=l1;, score=0.781 total time=   0.4s\n[CV 3/5] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=l1;, score=0.765 total time=   0.4s\n[CV 5/5] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=l1;, score=0.788 total time=   0.3s\n[CV 2/5] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=elasticnet;, score=0.580 total time=   0.3s\n[CV 5/5] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=elasticnet;, score=0.823 total time=   0.4s\n[CV 2/5] END alpha=0.001, loss=squared_hinge, max_iter=10000, penalty=l2;, score=0.603 total time=   0.1s\n[CV 4/5] END alpha=0.001, loss=squared_hinge, max_iter=10000, penalty=l2;, score=0.578 total time=   0.1s\n[CV 1/5] END alpha=0.001, loss=squared_hinge, max_iter=10000, penalty=l1;, score=0.689 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=squared_hinge, max_iter=10000, penalty=l1;, score=0.777 total time=   0.3s\n[CV 5/5] END alpha=0.001, loss=squared_hinge, max_iter=10000, penalty=l1;, score=0.760 total time=   0.3s\n[CV 1/5] END alpha=0.001, loss=squared_hinge, max_iter=10000, penalty=elasticnet;, score=0.821 total time=   0.5s\n[CV 4/5] END alpha=0.001, loss=squared_hinge, max_iter=10000, penalty=elasticnet;, score=0.749 total time=   0.2s\n[CV 5/5] END alpha=0.001, loss=squared_hinge, max_iter=10000, penalty=elasticnet;, score=0.514 total time=   0.3s\n[CV 3/5] END alpha=0.001, loss=squared_hinge, max_iter=30000, penalty=l2;, score=0.583 total time=   0.1s\n[CV 4/5] END alpha=0.001, loss=squared_hinge, max_iter=30000, penalty=l2;, score=0.578 total time=   0.1s\n[CV 1/5] END alpha=0.001, loss=squared_hinge, max_iter=30000, penalty=l1;, score=0.756 total time=   0.4s\n[CV 3/5] END alpha=0.001, loss=squared_hinge, max_iter=30000, penalty=l1;, score=0.662 total time=   0.2s\n[CV 5/5] END alpha=0.001, loss=squared_hinge, max_iter=30000, penalty=l1;, score=0.711 total time=   0.3s\n[CV 2/5] END alpha=0.001, loss=squared_hinge, max_iter=30000, penalty=elasticnet;, score=0.489 total time=   0.2s\n[CV 3/5] END alpha=0.001, loss=squared_hinge, max_iter=30000, penalty=elasticnet;, score=0.473 total time=   0.3s\n[CV 5/5] END alpha=0.001, loss=squared_hinge, max_iter=30000, penalty=elasticnet;, score=0.611 total time=   0.4s\n[CV 2/5] END alpha=0.001, loss=perceptron, max_iter=1000, penalty=l2;, score=0.608 total time=   0.2s\n[CV 5/5] END alpha=0.001, loss=perceptron, max_iter=1000, penalty=l2;, score=0.578 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=perceptron, max_iter=1000, penalty=l1;, score=0.796 total time=   0.2s\n[CV 3/5] END alpha=0.001, loss=perceptron, max_iter=1000, penalty=l1;, score=0.791 total time=   0.2s\n[CV 5/5] END alpha=0.001, loss=perceptron, max_iter=1000, penalty=l1;, score=0.797 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=perceptron, max_iter=1000, penalty=elasticnet;, score=0.853 total time=   0.3s\n[CV 4/5] END alpha=0.001, loss=perceptron, max_iter=1000, penalty=elasticnet;, score=0.832 total time=   0.2s\n[CV 1/5] END alpha=0.001, loss=perceptron, max_iter=10000, penalty=l2;, score=0.484 total time=   0.2s\n[CV 3/5] END alpha=0.001, loss=perceptron, max_iter=10000, penalty=l2;, score=0.817 total time=   0.2s\n[CV 5/5] END alpha=0.001, loss=perceptron, max_iter=10000, penalty=l2;, score=0.784 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=perceptron, max_iter=10000, penalty=l1;, score=0.664 total time=   0.3s\n[CV 4/5] END alpha=0.001, loss=perceptron, max_iter=10000, penalty=l1;, score=0.793 total time=   0.2s\n[CV 1/5] END alpha=0.001, loss=perceptron, max_iter=10000, penalty=elasticnet;, score=0.756 total time=   0.3s\n[CV 3/5] END alpha=0.001, loss=perceptron, max_iter=10000, penalty=elasticnet;, score=0.799 total time=   0.4s\n[CV 1/5] END alpha=0.001, loss=perceptron, max_iter=30000, penalty=l2;, score=0.615 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=perceptron, max_iter=30000, penalty=l2;, score=0.634 total time=   0.1s\n[CV 3/5] END alpha=0.001, loss=perceptron, max_iter=30000, penalty=l2;, score=0.548 total time=   0.2s\n[CV 5/5] END alpha=0.001, loss=perceptron, max_iter=30000, penalty=l2;, score=0.778 total time=   0.1s\n[CV 2/5] END alpha=0.001, loss=perceptron, max_iter=30000, penalty=l1;, score=0.809 total time=   0.2s\n[CV 4/5] END alpha=0.001, loss=perceptron, max_iter=30000, penalty=l1;, score=0.779 total time=   0.2s\n[CV 1/5] END alpha=0.001, loss=perceptron, max_iter=30000, penalty=elasticnet;, score=0.861 total time=   0.4s\n[CV 3/5] END alpha=0.001, loss=perceptron, max_iter=30000, penalty=elasticnet;, score=0.705 total time=   0.3s\n[CV 5/5] END alpha=0.001, loss=perceptron, max_iter=30000, penalty=elasticnet;, score=0.634 total time=   0.3s\n[CV 3/5] END alpha=0.01, loss=hinge, max_iter=1000, penalty=l2;, score=0.568 total time=   0.2s\n"},"6":{"name":"stdout","text":"[CV 1/5] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=l2;, score=0.636 total time=   0.3s\n[CV 3/5] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=l2;, score=0.610 total time=   0.1s\n[CV 4/5] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=l2;, score=0.474 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=l1;, score=0.819 total time=   0.4s\n[CV 4/5] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=l1;, score=0.778 total time=   0.3s\n[CV 1/5] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=elasticnet;, score=0.523 total time=   0.4s\n[CV 3/5] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=elasticnet;, score=0.826 total time=   0.3s\n[CV 5/5] END alpha=0.001, loss=modified_huber, max_iter=1000, penalty=elasticnet;, score=0.851 total time=   0.5s\n[CV 3/5] END alpha=0.001, loss=modified_huber, max_iter=10000, penalty=l2;, score=0.799 total time=   0.1s\n[CV 5/5] END alpha=0.001, loss=modified_huber, max_iter=10000, penalty=l2;, score=0.673 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=modified_huber, max_iter=10000, penalty=l1;, score=0.727 total time=   0.4s\n[CV 5/5] END alpha=0.001, loss=modified_huber, max_iter=10000, penalty=l1;, score=0.669 total time=   0.3s\n[CV 1/5] END alpha=0.001, loss=modified_huber, max_iter=10000, penalty=elasticnet;, score=0.859 total time=   0.7s\n[CV 4/5] END alpha=0.001, loss=modified_huber, max_iter=10000, penalty=elasticnet;, score=0.567 total time=   0.3s\n[CV 1/5] END alpha=0.001, loss=modified_huber, max_iter=30000, penalty=l2;, score=0.635 total time=   0.1s\n[CV 3/5] END alpha=0.001, loss=modified_huber, max_iter=30000, penalty=l2;, score=0.681 total time=   0.2s\n[CV 5/5] END alpha=0.001, loss=modified_huber, max_iter=30000, penalty=l2;, score=0.525 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=modified_huber, max_iter=30000, penalty=l1;, score=0.697 total time=   0.5s\n[CV 4/5] END alpha=0.001, loss=modified_huber, max_iter=30000, penalty=l1;, score=0.780 total time=   0.3s\n[CV 1/5] END alpha=0.001, loss=modified_huber, max_iter=30000, penalty=elasticnet;, score=0.577 total time=   0.4s\n[CV 3/5] END alpha=0.001, loss=modified_huber, max_iter=30000, penalty=elasticnet;, score=0.569 total time=   0.5s\n[CV 5/5] END alpha=0.001, loss=modified_huber, max_iter=30000, penalty=elasticnet;, score=0.836 total time=   0.4s\n[CV 4/5] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=l2;, score=0.688 total time=   0.2s\n[CV 5/5] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=l2;, score=0.747 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=l1;, score=0.734 total time=   0.3s\n[CV 4/5] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=l1;, score=0.699 total time=   0.3s\n[CV 1/5] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=elasticnet;, score=0.629 total time=   0.3s\n[CV 3/5] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=elasticnet;, score=0.569 total time=   0.2s\n[CV 4/5] END alpha=0.001, loss=squared_hinge, max_iter=1000, penalty=elasticnet;, score=0.567 total time=   0.3s\n[CV 1/5] END alpha=0.001, loss=squared_hinge, max_iter=10000, penalty=l2;, score=0.664 total time=   0.2s\n[CV 3/5] END alpha=0.001, loss=squared_hinge, max_iter=10000, penalty=l2;, score=0.647 total time=   0.2s\n[CV 5/5] END alpha=0.001, loss=squared_hinge, max_iter=10000, penalty=l2;, score=0.758 total time=   0.2s\n[CV 3/5] END alpha=0.001, loss=squared_hinge, max_iter=10000, penalty=l1;, score=0.739 total time=   0.3s\n[CV 4/5] END alpha=0.001, loss=squared_hinge, max_iter=10000, penalty=l1;, score=0.738 total time=   0.3s\n[CV 2/5] END alpha=0.001, loss=squared_hinge, max_iter=10000, penalty=elasticnet;, score=0.649 total time=   0.3s\n[CV 3/5] END alpha=0.001, loss=squared_hinge, max_iter=10000, penalty=elasticnet;, score=0.676 total time=   0.5s\n[CV 1/5] END alpha=0.001, loss=squared_hinge, max_iter=30000, penalty=l2;, score=0.747 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=squared_hinge, max_iter=30000, penalty=l2;, score=0.684 total time=   0.2s\n[CV 5/5] END alpha=0.001, loss=squared_hinge, max_iter=30000, penalty=l2;, score=0.741 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=squared_hinge, max_iter=30000, penalty=l1;, score=0.807 total time=   0.4s\n[CV 4/5] END alpha=0.001, loss=squared_hinge, max_iter=30000, penalty=l1;, score=0.776 total time=   0.3s\n[CV 1/5] END alpha=0.001, loss=squared_hinge, max_iter=30000, penalty=elasticnet;, score=0.821 total time=   0.4s\n[CV 4/5] END alpha=0.001, loss=squared_hinge, max_iter=30000, penalty=elasticnet;, score=0.543 total time=   0.5s\n[CV 1/5] END alpha=0.001, loss=perceptron, max_iter=1000, penalty=l2;, score=0.757 total time=   0.2s\n[CV 3/5] END alpha=0.001, loss=perceptron, max_iter=1000, penalty=l2;, score=0.597 total time=   0.1s\n[CV 4/5] END alpha=0.001, loss=perceptron, max_iter=1000, penalty=l2;, score=0.699 total time=   0.2s\n[CV 1/5] END alpha=0.001, loss=perceptron, max_iter=1000, penalty=l1;, score=0.803 total time=   0.2s\n[CV 4/5] END alpha=0.001, loss=perceptron, max_iter=1000, penalty=l1;, score=0.778 total time=   0.2s\n[CV 1/5] END alpha=0.001, loss=perceptron, max_iter=1000, penalty=elasticnet;, score=0.849 total time=   0.3s\n[CV 3/5] END alpha=0.001, loss=perceptron, max_iter=1000, penalty=elasticnet;, score=0.840 total time=   0.3s\n[CV 5/5] END alpha=0.001, loss=perceptron, max_iter=1000, penalty=elasticnet;, score=0.631 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=perceptron, max_iter=10000, penalty=l2;, score=0.668 total time=   0.2s\n[CV 4/5] END alpha=0.001, loss=perceptron, max_iter=10000, penalty=l2;, score=0.807 total time=   0.2s\n[CV 1/5] END alpha=0.001, loss=perceptron, max_iter=10000, penalty=l1;, score=0.794 total time=   0.3s\n[CV 3/5] END alpha=0.001, loss=perceptron, max_iter=10000, penalty=l1;, score=0.792 total time=   0.2s\n[CV 5/5] END alpha=0.001, loss=perceptron, max_iter=10000, penalty=l1;, score=0.797 total time=   0.3s\n[CV 2/5] END alpha=0.001, loss=perceptron, max_iter=10000, penalty=elasticnet;, score=0.772 total time=   0.2s\n[CV 4/5] END alpha=0.001, loss=perceptron, max_iter=10000, penalty=elasticnet;, score=0.843 total time=   0.3s\n[CV 5/5] END alpha=0.001, loss=perceptron, max_iter=10000, penalty=elasticnet;, score=0.535 total time=   0.4s\n[CV 4/5] END alpha=0.001, loss=perceptron, max_iter=30000, penalty=l2;, score=0.605 total time=   0.2s\n[CV 1/5] END alpha=0.001, loss=perceptron, max_iter=30000, penalty=l1;, score=0.694 total time=   0.2s\n[CV 3/5] END alpha=0.001, loss=perceptron, max_iter=30000, penalty=l1;, score=0.797 total time=   0.2s\n[CV 5/5] END alpha=0.001, loss=perceptron, max_iter=30000, penalty=l1;, score=0.782 total time=   0.2s\n[CV 2/5] END alpha=0.001, loss=perceptron, max_iter=30000, penalty=elasticnet;, score=0.865 total time=   0.4s\n[CV 4/5] END alpha=0.001, loss=perceptron, max_iter=30000, penalty=elasticnet;, score=0.845 total time=   0.3s\n[CV 1/5] END alpha=0.01, loss=hinge, max_iter=1000, penalty=l2;, score=0.816 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=hinge, max_iter=1000, penalty=l2;, score=0.608 total time=   0.1s\n[CV 4/5] END alpha=0.01, loss=hinge, max_iter=1000, penalty=l2;, score=0.523 total time=   0.2s\n[CV 1/5] END alpha=0.01, loss=hinge, max_iter=1000, penalty=l1;, score=0.773 total time=   0.1s\n[CV 2/5] END alpha=0.01, loss=hinge, max_iter=1000, penalty=l1;, score=0.820 total time=   0.2s\n[CV 4/5] END alpha=0.01, loss=hinge, max_iter=1000, penalty=l1;, score=0.779 total time=   0.3s\n[CV 1/5] END alpha=0.01, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.827 total time=   0.1s\n[CV 3/5] END alpha=0.01, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.631 total time=   0.1s\n[CV 5/5] END alpha=0.01, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.822 total time=   0.2s\n[CV 3/5] END alpha=0.01, loss=hinge, max_iter=10000, penalty=l2;, score=0.546 total time=   0.1s\n[CV 4/5] END alpha=0.01, loss=hinge, max_iter=10000, penalty=l2;, score=0.621 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=hinge, max_iter=10000, penalty=l1;, score=0.788 total time=   0.1s\n"},"7":{"name":"stdout","text":"[CV 5/5] END alpha=0.01, loss=hinge, max_iter=1000, penalty=l2;, score=0.597 total time=   0.2s\n[CV 3/5] END alpha=0.01, loss=hinge, max_iter=1000, penalty=l1;, score=0.791 total time=   0.3s\n[CV 5/5] END alpha=0.01, loss=hinge, max_iter=1000, penalty=l1;, score=0.763 total time=   0.1s\n[CV 2/5] END alpha=0.01, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.809 total time=   0.2s\n[CV 4/5] END alpha=0.01, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.655 total time=   0.1s\n[CV 1/5] END alpha=0.01, loss=hinge, max_iter=10000, penalty=l2;, score=0.567 total time=   0.1s\n[CV 2/5] END alpha=0.01, loss=hinge, max_iter=10000, penalty=l2;, score=0.800 total time=   0.2s\n[CV 5/5] END alpha=0.01, loss=hinge, max_iter=10000, penalty=l2;, score=0.813 total time=   0.1s\n[CV 1/5] END alpha=0.01, loss=hinge, max_iter=10000, penalty=l1;, score=0.798 total time=   0.1s\n[CV 3/5] END alpha=0.01, loss=hinge, max_iter=10000, penalty=l1;, score=0.790 total time=   0.1s\n[CV 5/5] END alpha=0.01, loss=hinge, max_iter=10000, penalty=l1;, score=0.798 total time=   0.1s\n[CV 2/5] END alpha=0.01, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.862 total time=   0.2s\n[CV 4/5] END alpha=0.01, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.771 total time=   0.2s\n[CV 1/5] END alpha=0.01, loss=hinge, max_iter=30000, penalty=l2;, score=0.822 total time=   0.2s\n[CV 3/5] END alpha=0.01, loss=hinge, max_iter=30000, penalty=l2;, score=0.468 total time=   0.2s\n[CV 5/5] END alpha=0.01, loss=hinge, max_iter=30000, penalty=l2;, score=0.715 total time=   0.1s\n[CV 3/5] END alpha=0.01, loss=hinge, max_iter=30000, penalty=l1;, score=0.791 total time=   0.1s\n[CV 4/5] END alpha=0.01, loss=hinge, max_iter=30000, penalty=l1;, score=0.778 total time=   0.2s\n[CV 1/5] END alpha=0.01, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.805 total time=   0.1s\n[CV 3/5] END alpha=0.01, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.460 total time=   0.1s\n[CV 5/5] END alpha=0.01, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.746 total time=   0.1s\n[CV 2/5] END alpha=0.01, loss=log_loss, max_iter=1000, penalty=l2;, score=0.665 total time=   0.3s\n[CV 4/5] END alpha=0.01, loss=log_loss, max_iter=1000, penalty=l2;, score=0.516 total time=   0.3s\n[CV 1/5] END alpha=0.01, loss=log_loss, max_iter=1000, penalty=l1;, score=0.799 total time=   0.2s\n[CV 3/5] END alpha=0.01, loss=log_loss, max_iter=1000, penalty=l1;, score=0.786 total time=   0.1s\n[CV 5/5] END alpha=0.01, loss=log_loss, max_iter=1000, penalty=l1;, score=0.786 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=log_loss, max_iter=1000, penalty=elasticnet;, score=0.832 total time=   0.2s\n[CV 5/5] END alpha=0.01, loss=log_loss, max_iter=1000, penalty=elasticnet;, score=0.516 total time=   0.1s\n[CV 1/5] END alpha=0.01, loss=log_loss, max_iter=10000, penalty=l2;, score=0.619 total time=   0.2s\n[CV 3/5] END alpha=0.01, loss=log_loss, max_iter=10000, penalty=l2;, score=0.816 total time=   0.3s\n[CV 5/5] END alpha=0.01, loss=log_loss, max_iter=10000, penalty=l2;, score=0.482 total time=   0.3s\n[CV 2/5] END alpha=0.01, loss=log_loss, max_iter=10000, penalty=l1;, score=0.810 total time=   0.5s\n[CV 5/5] END alpha=0.01, loss=log_loss, max_iter=10000, penalty=l1;, score=0.766 total time=   0.1s\n[CV 2/5] END alpha=0.01, loss=log_loss, max_iter=10000, penalty=elasticnet;, score=0.647 total time=   0.2s\n[CV 4/5] END alpha=0.01, loss=log_loss, max_iter=10000, penalty=elasticnet;, score=0.853 total time=   0.4s\n[CV 1/5] END alpha=0.01, loss=log_loss, max_iter=30000, penalty=l2;, score=0.509 total time=   0.2s\n[CV 3/5] END alpha=0.01, loss=log_loss, max_iter=30000, penalty=l2;, score=0.571 total time=   0.2s\n[CV 5/5] END alpha=0.01, loss=log_loss, max_iter=30000, penalty=l2;, score=0.807 total time=   0.3s\n[CV 1/5] END alpha=0.01, loss=log_loss, max_iter=30000, penalty=l1;, score=0.779 total time=   0.1s\n[CV 3/5] END alpha=0.01, loss=log_loss, max_iter=30000, penalty=l1;, score=0.789 total time=   0.3s\n[CV 1/5] END alpha=0.01, loss=log_loss, max_iter=30000, penalty=elasticnet;, score=0.772 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=log_loss, max_iter=30000, penalty=elasticnet;, score=0.719 total time=   0.2s\n[CV 4/5] END alpha=0.01, loss=log_loss, max_iter=30000, penalty=elasticnet;, score=0.706 total time=   0.2s\n[CV 1/5] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=l2;, score=0.567 total time=   0.2s\n[CV 3/5] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=l2;, score=0.745 total time=   0.2s\n[CV 5/5] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=l2;, score=0.478 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=l1;, score=0.809 total time=   0.1s\n[CV 4/5] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=l1;, score=0.783 total time=   0.2s\n[CV 1/5] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=elasticnet;, score=0.853 total time=   0.3s\n[CV 4/5] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=elasticnet;, score=0.782 total time=   0.1s\n[CV 5/5] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=elasticnet;, score=0.839 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=modified_huber, max_iter=10000, penalty=l2;, score=0.778 total time=   0.3s\n[CV 4/5] END alpha=0.01, loss=modified_huber, max_iter=10000, penalty=l2;, score=0.599 total time=   0.2s\n[CV 1/5] END alpha=0.01, loss=modified_huber, max_iter=10000, penalty=l1;, score=0.725 total time=   0.1s\n[CV 3/5] END alpha=0.01, loss=modified_huber, max_iter=10000, penalty=l1;, score=0.785 total time=   0.2s\n[CV 5/5] END alpha=0.01, loss=modified_huber, max_iter=10000, penalty=l1;, score=0.799 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=modified_huber, max_iter=10000, penalty=elasticnet;, score=0.750 total time=   0.2s\n[CV 4/5] END alpha=0.01, loss=modified_huber, max_iter=10000, penalty=elasticnet;, score=0.658 total time=   0.2s\n[CV 1/5] END alpha=0.01, loss=modified_huber, max_iter=30000, penalty=l2;, score=0.661 total time=   0.1s\n[CV 3/5] END alpha=0.01, loss=modified_huber, max_iter=30000, penalty=l2;, score=0.572 total time=   0.1s\n[CV 5/5] END alpha=0.01, loss=modified_huber, max_iter=30000, penalty=l2;, score=0.729 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=modified_huber, max_iter=30000, penalty=l1;, score=0.814 total time=   0.2s\n[CV 5/5] END alpha=0.01, loss=modified_huber, max_iter=30000, penalty=l1;, score=0.791 total time=   0.2s\n[CV 1/5] END alpha=0.01, loss=modified_huber, max_iter=30000, penalty=elasticnet;, score=0.859 total time=   0.2s\n[CV 3/5] END alpha=0.01, loss=modified_huber, max_iter=30000, penalty=elasticnet;, score=0.681 total time=   0.2s\n[CV 5/5] END alpha=0.01, loss=modified_huber, max_iter=30000, penalty=elasticnet;, score=0.828 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=l2;, score=0.770 total time=   0.1s\n[CV 4/5] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=l2;, score=0.591 total time=   0.2s\n[CV 5/5] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=l2;, score=0.572 total time=   0.1s\n[CV 2/5] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=l1;, score=0.784 total time=   0.4s\n[CV 4/5] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=l1;, score=0.665 total time=   0.3s\n[CV 1/5] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=elasticnet;, score=0.568 total time=   0.3s\n[CV 3/5] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=elasticnet;, score=0.793 total time=   0.4s\n[CV 5/5] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=elasticnet;, score=0.796 total time=   0.4s\n[CV 3/5] END alpha=0.01, loss=squared_hinge, max_iter=10000, penalty=l2;, score=0.765 total time=   0.2s\n[CV 5/5] END alpha=0.01, loss=squared_hinge, max_iter=10000, penalty=l2;, score=0.503 total time=   0.2s\n[CV 1/5] END alpha=0.01, loss=squared_hinge, max_iter=10000, penalty=l1;, score=0.781 total time=   0.4s\n[CV 4/5] END alpha=0.01, loss=squared_hinge, max_iter=10000, penalty=l1;, score=0.761 total time=   0.2s\n"},"8":{"name":"stdout","text":"[CV 4/5] END alpha=0.01, loss=hinge, max_iter=10000, penalty=l1;, score=0.768 total time=   0.2s\n[CV 1/5] END alpha=0.01, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.729 total time=   0.1s\n[CV 3/5] END alpha=0.01, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.730 total time=   0.2s\n[CV 5/5] END alpha=0.01, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.807 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=hinge, max_iter=30000, penalty=l2;, score=0.493 total time=   0.1s\n[CV 4/5] END alpha=0.01, loss=hinge, max_iter=30000, penalty=l2;, score=0.569 total time=   0.2s\n[CV 1/5] END alpha=0.01, loss=hinge, max_iter=30000, penalty=l1;, score=0.791 total time=   0.1s\n[CV 2/5] END alpha=0.01, loss=hinge, max_iter=30000, penalty=l1;, score=0.791 total time=   0.2s\n[CV 5/5] END alpha=0.01, loss=hinge, max_iter=30000, penalty=l1;, score=0.603 total time=   0.1s\n[CV 2/5] END alpha=0.01, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.768 total time=   0.2s\n[CV 4/5] END alpha=0.01, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.449 total time=   0.1s\n[CV 1/5] END alpha=0.01, loss=log_loss, max_iter=1000, penalty=l2;, score=0.785 total time=   0.3s\n[CV 3/5] END alpha=0.01, loss=log_loss, max_iter=1000, penalty=l2;, score=0.467 total time=   0.3s\n[CV 5/5] END alpha=0.01, loss=log_loss, max_iter=1000, penalty=l2;, score=0.604 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=log_loss, max_iter=1000, penalty=l1;, score=0.778 total time=   0.1s\n[CV 4/5] END alpha=0.01, loss=log_loss, max_iter=1000, penalty=l1;, score=0.767 total time=   0.3s\n[CV 1/5] END alpha=0.01, loss=log_loss, max_iter=1000, penalty=elasticnet;, score=0.855 total time=   0.2s\n[CV 3/5] END alpha=0.01, loss=log_loss, max_iter=1000, penalty=elasticnet;, score=0.797 total time=   0.1s\n[CV 4/5] END alpha=0.01, loss=log_loss, max_iter=1000, penalty=elasticnet;, score=0.696 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=log_loss, max_iter=10000, penalty=l2;, score=0.777 total time=   0.3s\n[CV 4/5] END alpha=0.01, loss=log_loss, max_iter=10000, penalty=l2;, score=0.573 total time=   0.4s\n[CV 1/5] END alpha=0.01, loss=log_loss, max_iter=10000, penalty=l1;, score=0.794 total time=   0.3s\n[CV 3/5] END alpha=0.01, loss=log_loss, max_iter=10000, penalty=l1;, score=0.783 total time=   0.1s\n[CV 4/5] END alpha=0.01, loss=log_loss, max_iter=10000, penalty=l1;, score=0.784 total time=   0.3s\n[CV 1/5] END alpha=0.01, loss=log_loss, max_iter=10000, penalty=elasticnet;, score=0.847 total time=   0.3s\n[CV 3/5] END alpha=0.01, loss=log_loss, max_iter=10000, penalty=elasticnet;, score=0.636 total time=   0.2s\n[CV 5/5] END alpha=0.01, loss=log_loss, max_iter=10000, penalty=elasticnet;, score=0.833 total time=   0.3s\n[CV 2/5] END alpha=0.01, loss=log_loss, max_iter=30000, penalty=l2;, score=0.824 total time=   0.2s\n[CV 4/5] END alpha=0.01, loss=log_loss, max_iter=30000, penalty=l2;, score=0.601 total time=   0.3s\n[CV 2/5] END alpha=0.01, loss=log_loss, max_iter=30000, penalty=l1;, score=0.793 total time=   0.1s\n[CV 4/5] END alpha=0.01, loss=log_loss, max_iter=30000, penalty=l1;, score=0.781 total time=   0.1s\n[CV 5/5] END alpha=0.01, loss=log_loss, max_iter=30000, penalty=l1;, score=0.800 total time=   0.3s\n[CV 3/5] END alpha=0.01, loss=log_loss, max_iter=30000, penalty=elasticnet;, score=0.643 total time=   0.1s\n[CV 5/5] END alpha=0.01, loss=log_loss, max_iter=30000, penalty=elasticnet;, score=0.722 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=l2;, score=0.634 total time=   0.2s\n[CV 4/5] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=l2;, score=0.576 total time=   0.2s\n[CV 1/5] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=l1;, score=0.622 total time=   0.2s\n[CV 3/5] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=l1;, score=0.792 total time=   0.2s\n[CV 5/5] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=l1;, score=0.800 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=elasticnet;, score=0.853 total time=   0.2s\n[CV 3/5] END alpha=0.01, loss=modified_huber, max_iter=1000, penalty=elasticnet;, score=0.851 total time=   0.2s\n[CV 1/5] END alpha=0.01, loss=modified_huber, max_iter=10000, penalty=l2;, score=0.575 total time=   0.3s\n[CV 3/5] END alpha=0.01, loss=modified_huber, max_iter=10000, penalty=l2;, score=0.569 total time=   0.1s\n[CV 5/5] END alpha=0.01, loss=modified_huber, max_iter=10000, penalty=l2;, score=0.831 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=modified_huber, max_iter=10000, penalty=l1;, score=0.795 total time=   0.1s\n[CV 4/5] END alpha=0.01, loss=modified_huber, max_iter=10000, penalty=l1;, score=0.786 total time=   0.2s\n[CV 1/5] END alpha=0.01, loss=modified_huber, max_iter=10000, penalty=elasticnet;, score=0.855 total time=   0.3s\n[CV 3/5] END alpha=0.01, loss=modified_huber, max_iter=10000, penalty=elasticnet;, score=0.838 total time=   0.2s\n[CV 5/5] END alpha=0.01, loss=modified_huber, max_iter=10000, penalty=elasticnet;, score=0.831 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=modified_huber, max_iter=30000, penalty=l2;, score=0.567 total time=   0.2s\n[CV 4/5] END alpha=0.01, loss=modified_huber, max_iter=30000, penalty=l2;, score=0.586 total time=   0.2s\n[CV 1/5] END alpha=0.01, loss=modified_huber, max_iter=30000, penalty=l1;, score=0.796 total time=   0.1s\n[CV 3/5] END alpha=0.01, loss=modified_huber, max_iter=30000, penalty=l1;, score=0.763 total time=   0.1s\n[CV 4/5] END alpha=0.01, loss=modified_huber, max_iter=30000, penalty=l1;, score=0.775 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=modified_huber, max_iter=30000, penalty=elasticnet;, score=0.848 total time=   0.3s\n[CV 4/5] END alpha=0.01, loss=modified_huber, max_iter=30000, penalty=elasticnet;, score=0.853 total time=   0.3s\n[CV 1/5] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=l2;, score=0.812 total time=   0.2s\n[CV 3/5] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=l2;, score=0.839 total time=   0.3s\n[CV 1/5] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=l1;, score=0.766 total time=   0.2s\n[CV 3/5] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=l1;, score=0.760 total time=   0.3s\n[CV 5/5] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=l1;, score=0.773 total time=   0.3s\n[CV 2/5] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=elasticnet;, score=0.743 total time=   0.4s\n[CV 4/5] END alpha=0.01, loss=squared_hinge, max_iter=1000, penalty=elasticnet;, score=0.584 total time=   0.3s\n[CV 1/5] END alpha=0.01, loss=squared_hinge, max_iter=10000, penalty=l2;, score=0.500 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=squared_hinge, max_iter=10000, penalty=l2;, score=0.781 total time=   0.3s\n[CV 4/5] END alpha=0.01, loss=squared_hinge, max_iter=10000, penalty=l2;, score=0.487 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=squared_hinge, max_iter=10000, penalty=l1;, score=0.778 total time=   0.3s\n[CV 3/5] END alpha=0.01, loss=squared_hinge, max_iter=10000, penalty=l1;, score=0.774 total time=   0.4s\n[CV 1/5] END alpha=0.01, loss=squared_hinge, max_iter=10000, penalty=elasticnet;, score=0.567 total time=   0.3s\n[CV 3/5] END alpha=0.01, loss=squared_hinge, max_iter=10000, penalty=elasticnet;, score=0.807 total time=   0.2s\n[CV 5/5] END alpha=0.01, loss=squared_hinge, max_iter=10000, penalty=elasticnet;, score=0.604 total time=   0.4s\n[CV 1/5] END alpha=0.01, loss=squared_hinge, max_iter=30000, penalty=l2;, score=0.583 total time=   0.2s\n[CV 3/5] END alpha=0.01, loss=squared_hinge, max_iter=30000, penalty=l2;, score=0.603 total time=   0.3s\n[CV 4/5] END alpha=0.01, loss=squared_hinge, max_iter=30000, penalty=l2;, score=0.644 total time=   0.2s\n[CV 1/5] END alpha=0.01, loss=squared_hinge, max_iter=30000, penalty=l1;, score=0.758 total time=   0.4s\n[CV 3/5] END alpha=0.01, loss=squared_hinge, max_iter=30000, penalty=l1;, score=0.753 total time=   0.3s\n[CV 5/5] END alpha=0.01, loss=squared_hinge, max_iter=30000, penalty=l1;, score=0.755 total time=   0.2s\n"},"9":{"name":"stdout","text":"[CV 5/5] END alpha=0.01, loss=squared_hinge, max_iter=10000, penalty=l1;, score=0.784 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=squared_hinge, max_iter=10000, penalty=elasticnet;, score=0.579 total time=   0.3s\n[CV 4/5] END alpha=0.01, loss=squared_hinge, max_iter=10000, penalty=elasticnet;, score=0.828 total time=   0.7s\n[CV 2/5] END alpha=0.01, loss=squared_hinge, max_iter=30000, penalty=l2;, score=0.588 total time=   0.4s\n[CV 5/5] END alpha=0.01, loss=squared_hinge, max_iter=30000, penalty=l2;, score=0.592 total time=   0.3s\n[CV 2/5] END alpha=0.01, loss=squared_hinge, max_iter=30000, penalty=l1;, score=0.684 total time=   0.3s\n[CV 4/5] END alpha=0.01, loss=squared_hinge, max_iter=30000, penalty=l1;, score=0.762 total time=   0.3s\n[CV 1/5] END alpha=0.01, loss=squared_hinge, max_iter=30000, penalty=elasticnet;, score=0.796 total time=   0.4s\n[CV 3/5] END alpha=0.01, loss=squared_hinge, max_iter=30000, penalty=elasticnet;, score=0.598 total time=   0.4s\n[CV 5/5] END alpha=0.01, loss=squared_hinge, max_iter=30000, penalty=elasticnet;, score=0.572 total time=   0.4s\n[CV 2/5] END alpha=0.01, loss=perceptron, max_iter=1000, penalty=l2;, score=0.568 total time=   0.1s\n[CV 4/5] END alpha=0.01, loss=perceptron, max_iter=1000, penalty=l2;, score=0.626 total time=   0.2s\n[CV 1/5] END alpha=0.01, loss=perceptron, max_iter=1000, penalty=l1;, score=0.784 total time=   0.1s\n[CV 3/5] END alpha=0.01, loss=perceptron, max_iter=1000, penalty=l1;, score=0.790 total time=   0.1s\n[CV 4/5] END alpha=0.01, loss=perceptron, max_iter=1000, penalty=l1;, score=0.632 total time=   0.1s\n[CV 1/5] END alpha=0.01, loss=perceptron, max_iter=1000, penalty=elasticnet;, score=0.694 total time=   0.1s\n[CV 3/5] END alpha=0.01, loss=perceptron, max_iter=1000, penalty=elasticnet;, score=0.441 total time=   0.1s\n[CV 5/5] END alpha=0.01, loss=perceptron, max_iter=1000, penalty=elasticnet;, score=0.781 total time=   0.1s\n[CV 2/5] END alpha=0.01, loss=perceptron, max_iter=10000, penalty=l2;, score=0.575 total time=   0.2s\n[CV 4/5] END alpha=0.01, loss=perceptron, max_iter=10000, penalty=l2;, score=0.596 total time=   0.2s\n[CV 1/5] END alpha=0.01, loss=perceptron, max_iter=10000, penalty=l1;, score=0.794 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=perceptron, max_iter=10000, penalty=l1;, score=0.798 total time=   0.1s\n[CV 5/5] END alpha=0.01, loss=perceptron, max_iter=10000, penalty=l1;, score=0.785 total time=   0.2s\n[CV 2/5] END alpha=0.01, loss=perceptron, max_iter=10000, penalty=elasticnet;, score=0.849 total time=   0.2s\n[CV 4/5] END alpha=0.01, loss=perceptron, max_iter=10000, penalty=elasticnet;, score=0.850 total time=   0.2s\n[CV 1/5] END alpha=0.01, loss=perceptron, max_iter=30000, penalty=l2;, score=0.600 total time=   0.2s\n[CV 3/5] END alpha=0.01, loss=perceptron, max_iter=30000, penalty=l2;, score=0.597 total time=   0.2s\n[CV 5/5] END alpha=0.01, loss=perceptron, max_iter=30000, penalty=l2;, score=0.568 total time=   0.1s\n[CV 2/5] END alpha=0.01, loss=perceptron, max_iter=30000, penalty=l1;, score=0.796 total time=   0.2s\n[CV 4/5] END alpha=0.01, loss=perceptron, max_iter=30000, penalty=l1;, score=0.766 total time=   0.2s\n[CV 1/5] END alpha=0.01, loss=perceptron, max_iter=30000, penalty=elasticnet;, score=0.853 total time=   0.2s\n[CV 3/5] END alpha=0.01, loss=perceptron, max_iter=30000, penalty=elasticnet;, score=0.705 total time=   0.2s\n[CV 5/5] END alpha=0.01, loss=perceptron, max_iter=30000, penalty=elasticnet;, score=0.467 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=hinge, max_iter=1000, penalty=l2;, score=0.610 total time=   0.4s\n[CV 4/5] END alpha=0.1, loss=hinge, max_iter=1000, penalty=l2;, score=0.506 total time=   0.1s\n[CV 1/5] END alpha=0.1, loss=hinge, max_iter=1000, penalty=l1;, score=0.742 total time=   0.1s\n[CV 4/5] END alpha=0.1, loss=hinge, max_iter=1000, penalty=l1;, score=0.739 total time=   0.3s\n[CV 2/5] END alpha=0.1, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.608 total time=   0.2s\n[CV 4/5] END alpha=0.1, loss=hinge, max_iter=1000, penalty=elasticnet;, score=0.556 total time=   0.2s\n[CV 1/5] END alpha=0.1, loss=hinge, max_iter=10000, penalty=l2;, score=0.826 total time=   0.4s\n[CV 3/5] END alpha=0.1, loss=hinge, max_iter=10000, penalty=l2;, score=0.828 total time=   0.2s\n[CV 4/5] END alpha=0.1, loss=hinge, max_iter=10000, penalty=l2;, score=0.508 total time=   0.1s\n[CV 1/5] END alpha=0.1, loss=hinge, max_iter=10000, penalty=l1;, score=0.771 total time=   0.1s\n[CV 3/5] END alpha=0.1, loss=hinge, max_iter=10000, penalty=l1;, score=0.671 total time=   0.1s\n[CV 4/5] END alpha=0.1, loss=hinge, max_iter=10000, penalty=l1;, score=0.705 total time=   0.4s\n[CV 2/5] END alpha=0.1, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.482 total time=   0.2s\n[CV 4/5] END alpha=0.1, loss=hinge, max_iter=10000, penalty=elasticnet;, score=0.637 total time=   0.2s\n[CV 1/5] END alpha=0.1, loss=hinge, max_iter=30000, penalty=l2;, score=0.835 total time=   0.2s\n[CV 3/5] END alpha=0.1, loss=hinge, max_iter=30000, penalty=l2;, score=0.588 total time=   0.2s\n[CV 4/5] END alpha=0.1, loss=hinge, max_iter=30000, penalty=l2;, score=0.547 total time=   0.2s\n[CV 1/5] END alpha=0.1, loss=hinge, max_iter=30000, penalty=l1;, score=0.740 total time=   0.1s\n[CV 3/5] END alpha=0.1, loss=hinge, max_iter=30000, penalty=l1;, score=0.561 total time=   0.1s\n[CV 5/5] END alpha=0.1, loss=hinge, max_iter=30000, penalty=l1;, score=0.733 total time=   0.2s\n[CV 2/5] END alpha=0.1, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.571 total time=   0.2s\n[CV 4/5] END alpha=0.1, loss=hinge, max_iter=30000, penalty=elasticnet;, score=0.708 total time=   0.2s\n[CV 1/5] END alpha=0.1, loss=log_loss, max_iter=1000, penalty=l2;, score=0.654 total time=   0.3s\n[CV 3/5] END alpha=0.1, loss=log_loss, max_iter=1000, penalty=l2;, score=0.621 total time=   0.3s\n[CV 5/5] END alpha=0.1, loss=log_loss, max_iter=1000, penalty=l2;, score=0.504 total time=   0.4s\n[CV 2/5] END alpha=0.1, loss=log_loss, max_iter=1000, penalty=l1;, score=0.764 total time=   0.4s\n[CV 4/5] END alpha=0.1, loss=log_loss, max_iter=1000, penalty=l1;, score=0.566 total time=   0.3s\n[CV 1/5] END alpha=0.1, loss=log_loss, max_iter=1000, penalty=elasticnet;, score=0.673 total time=   0.2s\n[CV 3/5] END alpha=0.1, loss=log_loss, max_iter=1000, penalty=elasticnet;, score=0.630 total time=   0.2s\n[CV 5/5] END alpha=0.1, loss=log_loss, max_iter=1000, penalty=elasticnet;, score=0.755 total time=   0.3s\n[CV 2/5] END alpha=0.1, loss=log_loss, max_iter=10000, penalty=l2;, score=0.631 total time=   0.3s\n[CV 4/5] END alpha=0.1, loss=log_loss, max_iter=10000, penalty=l2;, score=0.623 total time=   0.5s\n[CV 3/5] END alpha=0.1, loss=log_loss, max_iter=10000, penalty=l1;, score=0.696 total time=   0.2s\n[CV 5/5] END alpha=0.1, loss=log_loss, max_iter=10000, penalty=l1;, score=0.515 total time=   0.2s\n[CV 1/5] END alpha=0.1, loss=log_loss, max_iter=10000, penalty=elasticnet;, score=0.733 total time=   0.3s\n[CV 4/5] END alpha=0.1, loss=log_loss, max_iter=10000, penalty=elasticnet;, score=0.609 total time=   0.3s\n[CV 1/5] END alpha=0.1, loss=log_loss, max_iter=30000, penalty=l2;, score=0.608 total time=   0.5s\n[CV 4/5] END alpha=0.1, loss=log_loss, max_iter=30000, penalty=l2;, score=0.564 total time=   0.2s\n[CV 1/5] END alpha=0.1, loss=log_loss, max_iter=30000, penalty=l1;, score=0.473 total time=   0.1s\n[CV 2/5] END alpha=0.1, loss=log_loss, max_iter=30000, penalty=l1;, score=0.705 total time=   0.3s\n[CV 5/5] END alpha=0.1, loss=log_loss, max_iter=30000, penalty=l1;, score=0.719 total time=   0.2s\n[CV 1/5] END alpha=0.1, loss=log_loss, max_iter=30000, penalty=elasticnet;, score=0.652 total time=   0.2s\n[CV 3/5] END alpha=0.1, loss=log_loss, max_iter=30000, penalty=elasticnet;, score=0.789 total time=   0.3s\n[CV 5/5] END alpha=0.1, loss=log_loss, max_iter=30000, penalty=elasticnet;, score=0.755 total time=   0.2s\n[CV 3/5] END alpha=0.1, loss=modified_huber, max_iter=1000, penalty=l2;, score=0.819 total time=   0.1s\n[CV 4/5] END alpha=0.1, loss=modified_huber, max_iter=1000, penalty=l2;, score=0.718 total time=   0.2s\n[CV 1/5] END alpha=0.1, loss=modified_huber, max_iter=1000, penalty=l1;, score=0.767 total time=   0.2s\n"}},"pos":12,"scrolled":true,"start":1657723067003,"state":"done","type":"cell"}
{"cell_type":"code","end":1657724107096,"exec_count":20,"id":"be6659","input":"NB_model = CategoricalNB(alpha = 3)\n\nNB_model.fit(x_train, y_train)\n\ny_pred_nb = NB_model.predict(x_test)\n\nrecall = sklearn.metrics.recall_score(y_test,y_pred_nb)\nprecision = sklearn.metrics.precision_score(y_test, y_pred_nb)\naccuracy = sklearn.metrics.accuracy_score(y_test, y_pred_nb)\nf_measure = sklearn.metrics.f1_score(y_test, y_pred_nb)\nprint(\"R: \", recall)\nprint(\"P: \", precision)\nprint(\"A: \", accuracy)\nprint(\"F: \", f_measure)\n\nsns.heatmap(confusion_matrix(y_test, y_pred_nb), annot=True, fmt='g')\n\nacc = accuracy_score(y_test, y_pred_nb)\nprec = precision_score(y_test, y_pred_nb)\nrecall = recall_score(y_test, y_pred_nb)\nf1 = f1_score(y_test, y_pred_nb)\nscores['nb'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\n# Ivan","kernel":"ds_env","output":{"0":{"name":"stdout","text":"R:  0.8657777777777778\nP:  0.9018518518518519\nA:  0.9007722007722008\nF:  0.8834467120181406\n"},"1":{"data":{"image/png":"109cbce672aa7ac916691495d256eb209d2c6eba","text/plain":"<Figure size 432x288 with 2 Axes>"},"metadata":{"needs_background":"light"}}},"pos":13,"scrolled":true,"start":1657724106928,"state":"done","type":"cell"}
{"cell_type":"code","end":1657725167207,"exec_count":32,"id":"9770c5","input":"RF_model = RandomForestClassifier() # n_estimators=30, criterion=\"entropy\"\n\nRF_model.fit(x_train, y_train)\n\ny_pred_rf = RF_model.predict(x_test)\n\nrecall = sklearn.metrics.recall_score(y_test, y_pred_rf)\nprecision = sklearn.metrics.precision_score(y_test, y_pred_rf)\naccuracy = sklearn.metrics.accuracy_score(y_test, y_pred_rf)\nf_measure = sklearn.metrics.f1_score(y_test, y_pred_rf)\nprint(\"R: \", recall)\nprint(\"P: \", precision)\nprint(\"A: \", accuracy)\nprint(\"F: \", f_measure)\n\nsns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='g')\n\n\nacc = accuracy_score(y_test, y_pred_rf)\nprec = precision_score(y_test, y_pred_rf)\nrecall = recall_score(y_test, y_pred_rf)\nf1 = f1_score(y_test, y_pred_rf)\nscores['rf'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}","kernel":"ds_env","output":{"0":{"name":"stdout","text":"R:  0.9422222222222222\nP:  0.9422222222222222\nA:  0.9498069498069498\nF:  0.9422222222222222\n"},"1":{"data":{"image/png":"77a5d7f0a3d2bd95b376e09f44e0a2a0e19df7dd","text/plain":"<Figure size 432x288 with 2 Axes>"},"metadata":{"needs_background":"light"}}},"pos":16,"scrolled":false,"start":1657725165898,"state":"done","type":"cell"}
{"cell_type":"code","end":1657725775949,"exec_count":35,"id":"84912c","input":"\n\n\nparams = {'min_samples_split': list(range(1,7)), 'min_samples_leaf': list(range(1,7)), 'min_weight_fraction_leaf': [-3.0,-2.0,-1.0,0.0,1.0,2.0,3.0], 'max_depth': list(range(1,6)), 'ccp_alpha': [0,0,1.0,2.0,3.0]}\ntree_gsearch = GSearch(DecisionTreeClassifier(max_depth=7), params)\ntree_gsearch.fit(x_train, y_train)\nfig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(tree_clf,\n                   feature_names=input_columns.columns,  \n                   class_names=['Not Satisfied','Satisfied'],\n                   filled=True, fontsize=10)\n\nbest_estimator = tree_gsearch.best_estimator_\ny_pred_tree = best_estimator.predict(x_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_tree))\nprint(\"Precision:\", precision_score(y_test, y_pred_tree))\nprint(\"Recall:\", recall_score(y_test, y_pred_tree))\nprint(\"F1 Score:\", f1_score(y_test, y_pred_tree))\nacc = accuracy_score(y_test, y_pred_tree)\nprec = precision_score(y_test, y_pred_tree)\nrecall = recall_score(y_test, y_pred_tree)\nf1 = f1_score(y_test, y_pred_tree)\nscores['tree grid'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}","kernel":"ds_env","output":{"0":{"name":"stderr","text":"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n27750 fits failed out of a total of 31500.\nThe score on these train-test partitions for these parameters will be set to nan.\nIf these failures are not expected, you can try to debug them by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5250 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 969, in fit\n    super().fit(\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 265, in fit\n    check_scalar(\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 1480, in check_scalar\n    raise ValueError(\nValueError: min_samples_split == 1, must be >= 2.\n\n--------------------------------------------------------------------------------\n3750 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 969, in fit\n    super().fit(\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 286, in fit\n    check_scalar(\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 1480, in check_scalar\n    raise ValueError(\nValueError: min_weight_fraction_leaf == -3.0, must be >= 0.0.\n\n--------------------------------------------------------------------------------\n3750 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 969, in fit\n    super().fit(\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 286, in fit\n    check_scalar(\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 1480, in check_scalar\n    raise ValueError(\nValueError: min_weight_fraction_leaf == -2.0, must be >= 0.0.\n\n--------------------------------------------------------------------------------\n3750 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 969, in fit\n    super().fit(\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 286, in fit\n    check_scalar(\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 1480, in check_scalar\n    raise ValueError(\nValueError: min_weight_fraction_leaf == -1.0, must be >= 0.0.\n\n--------------------------------------------------------------------------------\n3750 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 969, in fit\n    super().fit(\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 286, in fit\n    check_scalar(\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 1489, in check_scalar\n    raise ValueError(\nValueError: min_weight_fraction_leaf == 1.0, must be <= 0.5.\n\n--------------------------------------------------------------------------------\n3750 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 969, in fit\n    super().fit(\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 286, in fit\n    check_scalar(\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 1489, in check_scalar\n    raise ValueError(\nValueError: min_weight_fraction_leaf == 2.0, must be <= 0.5.\n\n--------------------------------------------------------------------------------\n3750 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 969, in fit\n    super().fit(\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 286, in fit\n    check_scalar(\n  File \"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 1489, in check_scalar\n    raise ValueError(\nValueError: min_weight_fraction_leaf == 3.0, must be <= 0.5.\n\n  warnings.warn(some_fits_failed_message, FitFailedWarning)\n/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n  warnings.warn(\n"},"1":{"name":"stdout","text":"Accuracy: 0.9158301158301159\nPrecision: 0.9126478616924477\nRecall: 0.8915555555555555\nF1 Score: 0.9019784172661871\n"},"2":{"data":{"image/png":"400328f37c173eb941c41224feab2d37ace2b4e5","text/plain":"<Figure size 1800x1440 with 1 Axes>"},"metadata":{"needs_background":"light"}}},"pos":18,"scrolled":true,"start":1657725557757,"state":"done","type":"cell"}
{"cell_type":"code","end":1657725775967,"exec_count":36,"id":"f98161","input":"print(scores)","kernel":"ds_env","output":{"0":{"name":"stdout","text":"{'sgd': {'accuracy': 0.6791505791505792, 'precision': 0.5888754534461911, 'recall': 0.8657777777777778, 'f1_score': 0.7009715725080965}, 'sgd grid': {'accuracy': 0.8555984555984556, 'precision': 0.8499534016775396, 'recall': 0.8106666666666666, 'f1_score': 0.8298453139217471}, 'nb': {'accuracy': 0.9007722007722008, 'precision': 0.9018518518518519, 'recall': 0.8657777777777778, 'f1_score': 0.8834467120181406}, 'rf': {'accuracy': 0.9498069498069498, 'precision': 0.9422222222222222, 'recall': 0.9422222222222222, 'f1_score': 0.9422222222222222}, 'tree': {'accuracy': 0.9293436293436294, 'precision': 0.9109947643979057, 'recall': 0.928, 'f1_score': 0.9194187582562747}, 'tree grid': {'accuracy': 0.9158301158301159, 'precision': 0.9126478616924477, 'recall': 0.8915555555555555, 'f1_score': 0.9019784172661871}}\n"}},"pos":19,"start":1657725775959,"state":"done","type":"cell"}
{"cell_type":"code","end":1657725879019,"exec_count":37,"id":"c0bce4","input":"\nadc_clf = ADC()\nadc_clf.fit(x_train, y_train)\ny_pred_adc = adc_clf.predict(x_test)\nacc = accuracy_score(y_test, y_pred_adc)\nprec = precision_score(y_test, y_pred_adc)\nrecall = recall_score(y_test, y_pred_adc)\nf1 = f1_score(y_test, y_pred_adc)\nscores['adc'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}","kernel":"ds_env","pos":27,"start":1657725878434,"state":"done","type":"cell"}
{"cell_type":"code","end":1657726491164,"exec_count":39,"id":"780f83","input":"\nlr_clf = LR()\nlr_clf.fit(x_train, y_train)\nlr_pred=lr_clf.predict(x_test)\n\ny_pred_lr = lr_clf.predict(x_test)\nacc = accuracy_score(y_test, y_pred_lr)\nprec = precision_score(y_test, y_pred_lr)\nrecall = recall_score(y_test, y_pred_lr)\nf1 = f1_score(y_test, y_pred_lr)\nscores['lr'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}","kernel":"ds_env","output":{"0":{"name":"stderr","text":"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n"}},"pos":32,"start":1657726491018,"state":"done","type":"cell"}
{"cell_type":"code","end":1657726667168,"exec_count":42,"id":"8652cd","input":"knn_clf = KNN()\nknn_clf.fit(x_train, y_train)\nknn_pred = knn_clf.predict(x_test)\ny_pred_knn = knn_clf.predict(x_test)\n\nax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, knn_pred), annot=True, fmt='g')\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix');\nax.xaxis.set_ticklabels(['0: Not Satisfied', '1: Satisfied',]); ax.yaxis.set_ticklabels(['0: Not Satisfied', '1: Satisfied'])\n\nrecall = sklearn.metrics.recall_score(y_test, knn_pred)\nprecision = sklearn.metrics.precision_score(y_test, knn_pred)\naccuracy = sklearn.metrics.accuracy_score(y_test, knn_pred)\nf_measure = sklearn.metrics.f1_score(y_test, knn_pred)\nprint(\"R: \", recall)\nprint(\"P: \", precision)\nprint(\"A: \", accuracy)\nprint(\"F: \", f_measure)\n\ny_pred_lr = knn_clf.predict(x_test)\nacc = accuracy_score(y_test, y_pred_knn)\nprec = precision_score(y_test, y_pred_knn)\nrecall = recall_score(y_test, y_pred_knn)\nf1 = f1_score(y_test, y_pred_knn)\nscores['knn'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\n# Sebastian ","kernel":"ds_env","output":{"0":{"name":"stdout","text":"R:  0.5751111111111111\nP:  0.6293774319066148\nA:  0.6683397683397684\nF:  0.6010218300046447\n"},"1":{"data":{"image/png":"614c5285da76bdbe130fe38a3b8182243015ab63","text/plain":"<Figure size 432x288 with 2 Axes>"},"metadata":{"needs_background":"light"}}},"pos":36,"start":1657726666178,"state":"done","type":"cell"}
{"cell_type":"code","end":1657726854024,"exec_count":44,"id":"f46092","input":"grid_params = {'n_neighbors':[1,2,3,4,5,6,7], 'leaf_size':[45] , 'p' :[4] }\n\ngs = GSearch(KNN(), grid_params)\ngs.fit(x_train, y_train)\n\nbest_estimator = gs.best_estimator_\ny_pred_knn = best_estimator.predict(x_test)\nacc = accuracy_score(y_test, y_pred_knn)\nprec = precision_score(y_test, y_pred_knn)\nrecall = recall_score(y_test, y_pred_knn)\nf1 = f1_score(y_test, y_pred_knn)\nscores['knn_grid'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\n","kernel":"ds_env","output":{"0":{"ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m grid_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_neighbors\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m7\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleaf_size\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m45\u001b[39m] , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m :[\u001b[38;5;241m4\u001b[39m] }\n\u001b[1;32m      3\u001b[0m gs \u001b[38;5;241m=\u001b[39m GSearch(KNN(), grid_params)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m best_estimator \u001b[38;5;241m=\u001b[39m gs\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m      7\u001b[0m y_pred_knn \u001b[38;5;241m=\u001b[39m best_estimator\u001b[38;5;241m.\u001b[39mpredict(x_test)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py:1375\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1375\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[0;32m--> 822\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n","File \u001b[0;32m~/miniconda3/envs/ds_env/lib/python3.8/site-packages/joblib/parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1046\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/ds_env/lib/python3.8/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/ds_env/lib/python3.8/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n","File \u001b[0;32m~/miniconda3/envs/ds_env/lib/python3.8/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n","File \u001b[0;32m~/miniconda3/envs/ds_env/lib/python3.8/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/ds_env/lib/python3.8/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n","File \u001b[0;32m~/miniconda3/envs/ds_env/lib/python3.8/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:708\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    705\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_error\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    707\u001b[0m fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m--> 708\u001b[0m test_scores \u001b[38;5;241m=\u001b[39m \u001b[43m_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m score_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time \u001b[38;5;241m-\u001b[39m fit_time\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_train_score:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:767\u001b[0m, in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, error_score)\u001b[0m\n\u001b[1;32m    765\u001b[0m         scores \u001b[38;5;241m=\u001b[39m scorer(estimator, X_test)\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 767\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mscorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m error_score \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_scorer.py:429\u001b[0m, in \u001b[0;36m_passthrough_scorer\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_passthrough_scorer\u001b[39m(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;124;03m\"\"\"Function that wraps estimator.score\"\"\"\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py:666\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;124;03m    Mean accuracy of ``self.predict(X)`` wrt. `y`.\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m--> 666\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy_score(y, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:219\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m\"\"\"Predict the class labels for the provided data.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    Class labels for each data sample.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# In that case, we do not need the distances to perform\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the weighting so we do not compute them.\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     neigh_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/neighbors/_base.py:763\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    756\u001b[0m use_pairwise_distances_reductions \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m PairwiseDistancesArgKmin\u001b[38;5;241m.\u001b[39mis_usable_for(\n\u001b[1;32m    759\u001b[0m         X \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_\n\u001b[1;32m    760\u001b[0m     )\n\u001b[1;32m    761\u001b[0m )\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[0;32m--> 763\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mPairwiseDistancesArgKmin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_params_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[1;32m    775\u001b[0m ):\n\u001b[1;32m    776\u001b[0m     results \u001b[38;5;241m=\u001b[39m _kneighbors_from_graph(\n\u001b[1;32m    777\u001b[0m         X, n_neighbors\u001b[38;5;241m=\u001b[39mn_neighbors, return_distance\u001b[38;5;241m=\u001b[39mreturn_distance\n\u001b[1;32m    778\u001b[0m     )\n","File \u001b[0;32msklearn/metrics/_pairwise_distances_reduction.pyx:691\u001b[0m, in \u001b[0;36msklearn.metrics._pairwise_distances_reduction.PairwiseDistancesArgKmin.compute\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m~/miniconda3/envs/ds_env/lib/python3.8/site-packages/threadpoolctl.py:171\u001b[0m, in \u001b[0;36m_ThreadpoolLimiter.__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m, value, traceback):\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_original_limits()\n\u001b[1;32m    174\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;28mcls\u001b[39m, controller, \u001b[38;5;241m*\u001b[39m, limits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}},"pos":37,"start":1657726732614,"state":"done","type":"cell"}
{"cell_type":"code","end":1657726872588,"exec_count":46,"id":"162dcf","input":"scores_df = pd.DataFrame(scores)\nscores_df = round(scores_df, 2)\n\ndata = []\nfor column in scores_df.columns:\n    data.append(scores_df[column].tolist())\nfig = go.Figure(data=[go.Table(header=dict(values=list(scores_df.columns)),\n                 cells=dict(values=data))\n                     ])\nfig.show()\n\n","kernel":"ds_env","output":{"0":{"data":{"iframe":"c57a41c64ca6c78901c3c8473ee49b294d3832b0"}}},"pos":41,"start":1657726872485,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"1a85c7","input":"scores_df = pd.DataFrame(scores)","pos":38,"type":"cell"}
{"cell_type":"code","exec_count":16,"id":"301065","input":"tree_clf = DecisionTreeClassifier()\n","pos":23,"type":"cell"}
{"cell_type":"code","exec_count":17,"id":"ead63d","input":"tree_clf.fit(x_train, y_train)","output":{"0":{"data":{"text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>","text/plain":"DecisionTreeClassifier()"},"execution_count":17,"metadata":{},"output_type":"execute_result"}},"pos":24,"type":"cell"}
{"cell_type":"code","exec_count":18,"id":"36c640","input":"","output":{"0":{"data":{"text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier()</pre></div></div></div></div></div>","text/plain":"AdaBoostClassifier()"},"execution_count":18,"metadata":{},"output_type":"execute_result"}},"pos":25,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":20,"id":"46cf7e","input":"ax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, adc_pred), annot=True, fmt='g')\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix');\nax.xaxis.set_ticklabels(['0: Not Satisfied', '1: Satisfied',]); ax.yaxis.set_ticklabels(['0: Not Satisfied', '1: Satisfied'])\n\nprint(\"P: \", precision)\nprint(\"A: \", accuracy)\n\n# Sebastian ","output":{"0":{"name":"stdout","output_type":"stream","text":"P:  0.971309578898658\nA:  0.9582979380647154\n"},"1":{"data":{"image/png":"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","text/plain":"<Figure size 432x288 with 2 Axes>"},"execution_count":20,"metadata":{"needs_background":"light"},"output_type":"execute_result"}},"pos":28,"type":"cell"}
{"cell_type":"code","exec_count":21,"id":"24704a","input":"total_squared_error = (np.sum((y_test - adc_pred)**2))\nmean_squared_error = total_squared_error/len(y_test)\nprint(mean_squared_error)","output":{"0":{"name":"stdout","output_type":"stream","text":"0.07375086879295699\n"}},"pos":29,"type":"cell"}
{"cell_type":"code","exec_count":22,"id":"5bbd8d","input":"","pos":30,"type":"cell"}
{"cell_type":"code","exec_count":23,"id":"05f4ca","input":"","output":{"0":{"name":"stderr","output_type":"stream","text":"/projects/72aec78b-4035-4aa5-8faa-4d7af1533e7b/miniconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n"},"1":{"data":{"text/html":"<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>","text/plain":"LogisticRegression()"},"execution_count":23,"metadata":{},"output_type":"execute_result"}},"pos":31,"type":"cell"}
{"cell_type":"code","exec_count":25,"id":"d34cef","input":"ax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, lr_pred), annot=True, fmt='g')\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix');\nax.xaxis.set_ticklabels(['0: Not Satisfied', '1: Satisfied',]); ax.yaxis.set_ticklabels(['0: Not Satisfied', '1: Satisfied'])\n\nrecall = sklearn.metrics.recall_score(y_test, lr_pred)\nprecision = sklearn.metrics.precision_score(y_test, lr_pred)\naccuracy = sklearn.metrics.accuracy_score(y_test, lr_pred)\nf_measure = sklearn.metrics.f1_score(y_test, lr_pred)\nprint(\"R: \", recall)\nprint(\"P: \", precision)\nprint(\"A: \", accuracy)\nprint(\"F: \", f_measure)\n\n\n# Sebastian ","output":{"0":{"name":"stdout","output_type":"stream","text":"R:  0.7779849090102086\nP:  0.7825\nA:  0.8093675187273148\nF:  0.7802359225461828\n"},"1":{"data":{"image/png":"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","text/plain":"<Figure size 432x288 with 2 Axes>"},"execution_count":25,"metadata":{"needs_background":"light"},"output_type":"execute_result"}},"pos":33,"type":"cell"}
{"cell_type":"code","exec_count":26,"id":"45f3a5","input":"\n","pos":35,"type":"cell"}
{"cell_type":"code","exec_count":30,"id":"ed5be9","input":"#svc = SVC(probability=False)","pos":39,"type":"cell"}
{"cell_type":"code","exec_count":31,"id":"44dd58","input":"#svc.fit(x_train, y_train)\n#svc_pred = svc.pred(x_test, y_test)","pos":40,"type":"cell"}
{"cell_type":"code","exec_count":35,"id":"3be55e","input":"#Neural_model = MLPClassifier(solver='lbfgs', alpha=0.1, hidden_layer_sizes=(20, 20), random_state=1)\n\n#Neural_model.fit(x_train, y_train)\n\n#Neural_hat = Neural_model.predict(x_test)\n\n#precision = sklearn.metrics.precision_score(y_test, Neural_hat)\n#accuracy = sklearn.metrics.accuracy_score(y_test, Neural_hat)\n#print(\"P: \", precision)\n#print(\"A: \", accuracy)\n\n#sns.heatmap(confusion_matrix(y_test, Neural_hat), annot=True, fmt='g')","pos":15,"type":"cell"}
{"cell_type":"code","exec_count":83,"id":"8106a0","input":"ax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, y_pred_tree), annot=True, fmt='g')\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix');\nax.xaxis.set_ticklabels(['0: Not Satisfied', '1: Satisfied',]); ax.yaxis.set_ticklabels(['0: Not Satisfied', '1: Satisfied'])\n\nprint(\"P: \", precision)\nprint(\"A: \", accuracy)","output":{"0":{"name":"stdout","output_type":"stream","text":"P:  0.7256364823277068\nA:  0.7518727314850567\n"},"1":{"data":{"image/png":"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","text/plain":"<Figure size 432x288 with 2 Axes>"},"execution_count":83,"metadata":{"needs_background":"light"},"output_type":"execute_result"}},"pos":21,"type":"cell"}
{"cell_type":"markdown","id":"0507e6","input":"**Adaboost**\n\nShort for adaptive boosting, Adaboost takes combines models together to create the best model possible. It takes a bunch of weak learners and combines them into strong learners. It takes a number of desicion trees during the data training period. As the first model is made the incorrect values are the input for the next model to be made.\n","pos":26,"type":"cell"}
{"cell_type":"markdown","id":"58bcb0","input":"**Random Forest**\n\nA random forest is a model that makes a given number of randomized decision trees and uses them to make predictions on the data by using averaging functions to combine its results from several decision trees, making a more accurate/realistic prediction. Randomizing decision trees allows the model to individually look for correlations between sections of the dataset, making predictions more intuitive, compared to looking at the entire dataset through one decision tree, where it is harder to find patterns when there are so many variables to consider. It then takes the accuracy of the models it makes and displays them on the heatmap so that we can see how accurate/precise the model is. Through some light hyperparameter tuning, we found that at around 20 decision trees, there was no more improvement in the performance of the model and subsequent additions to the amount of decision trees would just increase the processing time. There were also no differences in accuracy/precision when we tried different criterion such as \"entropy\" and \"log\\_loss\", so we decided that \"gini\", the default criterion, was good enough for the model.\n\n- What are the pros and cons of the random forest\n- [https://towardsdatascience.com/hyperparameter\\-tuning\\-the\\-random\\-forest\\-in\\-python\\-using\\-scikit\\-learn\\-28d2aa77dd74](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)\n\n","pos":17,"type":"cell"}
{"cell_type":"markdown","id":"96b8d7","input":"<u>What is a KNN:</u>\n\nA KNN is a classification model and can be used for both supervised and unsupervised machine learning. KNN works by finding the distance between a query and all the examples in the data. K is then used to select the number of examples closest to the query. the KNN then votes for the most frequent label. \n\n- - [https://towardsdatascience.com/machine\\-learning\\-basics\\-with\\-the\\-k\\-nearest\\-neighbors\\-algorithm\\-6a6e71d01761](https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761)\n- What does the K mean?\n  - [https://towardsdatascience.com/a\\-simple\\-introduction\\-to\\-k\\-nearest\\-neighbors\\-algorithm\\-b3519ed98e](https://towardsdatascience.com/a-simple-introduction-to-k-nearest-neighbors-algorithm-b3519ed98e)\n  - K means a parameter that shows the number of nearest neighbors. This is necessary for the voting process.\n\nGrid search:\n\n[https://scikit\\-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n\n[https://medium.com/@erikgreenj/k\\-neighbors\\-classifier\\-with\\-gridsearchcv\\-basics\\-3c445ddeb657](https://medium.com/@erikgreenj/k-neighbors-classifier-with-gridsearchcv-basics-3c445ddeb657)\n\n","pos":34,"type":"cell"}
{"cell_type":"markdown","id":"9a447c","input":"**Decision Tree:**\n\nA decision tree is a graph that classifies items and whether it is true or false to the question in the box. In this example, we use satisfied for true and false for dissatisfied.\n\n","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"b8495a","input":"a Decision Tree Classifier makes various questions to help determine what group a variable falls into. it goes from the top, and depending on how a question is answered it goes down a path to a different leaf in the tree. It goes all the way down and at the end it puts that variable into a group\n\n- What is a splitting criteron\n- What are the pros and cons?\n- \n\n","pos":22,"type":"cell"}
{"cell_type":"markdown","id":"c4c371","input":"**Categorical Naive Bayes**\n\nThe categorical Naive Bayes model works using the Bayes Theorem, which assumes that individual features are independent on each other, which works well with our dataset, as we can see through our correlation matrix that none of our features are very correlated. Applying the Bayes Theorem assigns a probability to each feature in the dataset, then uses those probabilities to predict a whole outcome. The categorical Native Bayes model works best for categorical data, which works well with our dataset, as we have a lot of categorical data in the form of 1\\-5 ratings for the airline's performance.\n\n","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"c80a0b","input":"<u>Models to make:</u>\n\n- knn\n- decision tree\n- Random forest\n- SVC \\(DON'T DO THIS ONE YET\\)\n- Logistic regression \n- Adaboost \n- \n\nScoring me\ntrics:\n\n- f1 score\n- Recall\n- Precision \n- Accuracy\n\n","pos":42,"type":"cell"}
{"end":1657722975820,"exec_count":13,"id":"e50310","input":"scores = {}","kernel":"ds_env","pos":11.5,"start":1657722975811,"state":"done","type":"cell"}
{"end":1657724021109,"exec_count":16,"id":"17f891","input":"scores","kernel":"ds_env","output":{"0":{"data":{"text/plain":"{'sgd': {'accuracy': 0.6791505791505792,\n  'precision': 0.5888754534461911,\n  'recall': 0.8657777777777778,\n  'f1_score': 0.7009715725080965},\n 'sgd grid': {'accuracy': 0.8555984555984556,\n  'precision': 0.8499534016775396,\n  'recall': 0.8106666666666666,\n  'f1_score': 0.8298453139217471}}"},"exec_count":16}},"pos":12.5,"start":1657724021093,"state":"done","type":"cell"}
{"end":1657724614153,"exec_count":28,"id":"1f13f2","input":"import pickle \n\ndef save_obj(obj, name):\n    with open(f'{name}.pkl', 'wb') as outp:\n        pickle.dump(obj, outp)\n        \ndef load_obj(name):\n    with open(f\"{name}.pkl\",'r') as file:\n        object_file = pickle.load(file)\n        return object_file","kernel":"ds_env","pos":13.5,"start":1657724614143,"state":"done","type":"cell"}
{"end":1657724705356,"exec_count":29,"id":"d097f7","input":"save_obj(scores, \"scores\")\ntest = load_obj(\"scores\")\nprint(test)","kernel":"ds_env","output":{"0":{"ename":"UnicodeDecodeError","evalue":"'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)","Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m save_obj(scores, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mload_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscores\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(test)\n","Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36mload_obj\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_obj\u001b[39m(name):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m----> 9\u001b[0m         object_file \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m object_file\n","File \u001b[0;32m~/miniconda3/envs/ds_env/lib/python3.8/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n","\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"]}},"pos":13.75,"start":1657724705316,"state":"done","type":"cell"}
{"end":1657725176764,"exec_count":33,"id":"cfef09","input":"scores","kernel":"ds_env","output":{"0":{"data":{"text/plain":"{'sgd': {'accuracy': 0.6791505791505792,\n  'precision': 0.5888754534461911,\n  'recall': 0.8657777777777778,\n  'f1_score': 0.7009715725080965},\n 'sgd grid': {'accuracy': 0.8555984555984556,\n  'precision': 0.8499534016775396,\n  'recall': 0.8106666666666666,\n  'f1_score': 0.8298453139217471},\n 'nb': {'accuracy': 0.9007722007722008,\n  'precision': 0.9018518518518519,\n  'recall': 0.8657777777777778,\n  'f1_score': 0.8834467120181406},\n 'rf': {'accuracy': 0.9498069498069498,\n  'precision': 0.9422222222222222,\n  'recall': 0.9422222222222222,\n  'f1_score': 0.9422222222222222}}"},"exec_count":33}},"pos":16.5,"start":1657725176760,"state":"done","type":"cell"}
{"end":1657725264026,"exec_count":34,"id":"afcce2","input":"tree_clf = DecisionTreeClassifier()\ntree_clf = tree_clf.fit(x_train, y_train)\ny_pred_tree = tree_clf.predict(x_test)\n\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_tree))\nprint(\"Precision:\", precision_score(y_test, y_pred_tree))\nprint(\"Recall:\", recall_score(y_test, y_pred_tree))\nprint(\"F1 Score:\", f1_score(y_test, y_pred_tree))\n\nacc = accuracy_score(y_test, y_pred_tree)\nprec = precision_score(y_test, y_pred_tree)\nrecall = recall_score(y_test, y_pred_tree)\nf1 = f1_score(y_test, y_pred_tree)\nscores['tree'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}","kernel":"ds_env","output":{"0":{"name":"stdout","text":"Accuracy: 0.9293436293436294\nPrecision: 0.9109947643979057\nRecall: 0.928\nF1 Score: 0.9194187582562747\n"}},"pos":17.5,"start":1657725263942,"state":"done","type":"cell"}
{"end":1657726498855,"exec_count":40,"id":"b97c73","input":"scores","kernel":"ds_env","output":{"0":{"data":{"text/plain":"{'sgd': {'accuracy': 0.6791505791505792,\n  'precision': 0.5888754534461911,\n  'recall': 0.8657777777777778,\n  'f1_score': 0.7009715725080965},\n 'sgd grid': {'accuracy': 0.8555984555984556,\n  'precision': 0.8499534016775396,\n  'recall': 0.8106666666666666,\n  'f1_score': 0.8298453139217471},\n 'nb': {'accuracy': 0.9007722007722008,\n  'precision': 0.9018518518518519,\n  'recall': 0.8657777777777778,\n  'f1_score': 0.8834467120181406},\n 'rf': {'accuracy': 0.9498069498069498,\n  'precision': 0.9422222222222222,\n  'recall': 0.9422222222222222,\n  'f1_score': 0.9422222222222222},\n 'tree': {'accuracy': 0.9293436293436294,\n  'precision': 0.9109947643979057,\n  'recall': 0.928,\n  'f1_score': 0.9194187582562747},\n 'tree grid': {'accuracy': 0.9158301158301159,\n  'precision': 0.9126478616924477,\n  'recall': 0.8915555555555555,\n  'f1_score': 0.9019784172661871},\n 'adc': {'accuracy': 0.9351351351351351,\n  'precision': 0.9215859030837005,\n  'recall': 0.9297777777777778,\n  'f1_score': 0.9256637168141593},\n 'lr': {'accuracy': 0.8158301158301158,\n  'precision': 0.7634146341463415,\n  'recall': 0.8346666666666667,\n  'f1_score': 0.7974522292993631}}"},"exec_count":40}},"pos":32.5,"start":1657726498851,"state":"done","type":"cell"}
{"end":1657726865240,"exec_count":45,"id":"a2af96","input":"scores","kernel":"ds_env","output":{"0":{"data":{"text/plain":"{'sgd': {'accuracy': 0.6791505791505792,\n  'precision': 0.5888754534461911,\n  'recall': 0.8657777777777778,\n  'f1_score': 0.7009715725080965},\n 'sgd grid': {'accuracy': 0.8555984555984556,\n  'precision': 0.8499534016775396,\n  'recall': 0.8106666666666666,\n  'f1_score': 0.8298453139217471},\n 'nb': {'accuracy': 0.9007722007722008,\n  'precision': 0.9018518518518519,\n  'recall': 0.8657777777777778,\n  'f1_score': 0.8834467120181406},\n 'rf': {'accuracy': 0.9498069498069498,\n  'precision': 0.9422222222222222,\n  'recall': 0.9422222222222222,\n  'f1_score': 0.9422222222222222},\n 'tree': {'accuracy': 0.9293436293436294,\n  'precision': 0.9109947643979057,\n  'recall': 0.928,\n  'f1_score': 0.9194187582562747},\n 'tree grid': {'accuracy': 0.9158301158301159,\n  'precision': 0.9126478616924477,\n  'recall': 0.8915555555555555,\n  'f1_score': 0.9019784172661871},\n 'adc': {'accuracy': 0.9351351351351351,\n  'precision': 0.9215859030837005,\n  'recall': 0.9297777777777778,\n  'f1_score': 0.9256637168141593},\n 'lr': {'accuracy': 0.8158301158301158,\n  'precision': 0.7634146341463415,\n  'recall': 0.8346666666666667,\n  'f1_score': 0.7974522292993631},\n 'knn': {'accuracy': 0.6683397683397684,\n  'precision': 0.6293774319066148,\n  'recall': 0.5751111111111111,\n  'f1_score': 0.6010218300046447}}"},"exec_count":45}},"pos":37.5,"start":1657726865215,"state":"done","type":"cell"}
{"end":1657726893602,"exec_count":48,"id":"d260d8","input":"fig.write_html(\"scores.html\")","kernel":"ds_env","pos":41.5,"start":1657726893575,"state":"done","type":"cell"}
{"id":0,"time":1657723963325,"type":"user"}
{"last_load":1657722129643,"type":"file"}